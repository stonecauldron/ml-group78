{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.seterr(over='raise');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "# preprocess data\n",
    "tX = preprocess_inputs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=0\n",
      "Current iteration=1000, the loss=0\n",
      "Current iteration=2000, the loss=0\n",
      "Current iteration=3000, the loss=0\n",
      "Current iteration=4000, the loss=0\n",
      "Current iteration=5000, the loss=0\n",
      "Current iteration=6000, the loss=0\n",
      "Current iteration=7000, the loss=0\n"
     ]
    }
   ],
   "source": [
    "from toolbox import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 10000\n",
    "gamma = 0.0000025\n",
    "batch_size = 100\n",
    "k = 4\n",
    "k_indices = build_k_indices(y, k, 1)\n",
    "\n",
    "\n",
    "mean_loss_tr = []\n",
    "mean_loss_te = []\n",
    "\n",
    "lambdas = np.logspace(-4, 2, 30)\n",
    "for lambda_ in lambdas:\n",
    "    loss_tr = []\n",
    "    loss_te = []\n",
    "    for k_ in range(k):\n",
    "\n",
    "        test_indices = k_indices[k_]\n",
    "        test_y, test_x = (y[test_indices], tX[test_indices])\n",
    "\n",
    "        training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "        training_y, training_x = (y[training_indices], tX[training_indices])\n",
    "\n",
    "        w_star = reg_logistic_regression(training_y, training_x, lambda_, gamma, max_iters)\n",
    "\n",
    "        loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "        loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "    mean_loss_tr.append(np.mean(loss_tr))\n",
    "    mean_loss_te.append(np.mean(loss_te))\n",
    "#print(\"Training error: {tr}\\nTest error: {te}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis\n",
    "\n",
    "#### experiment 1\n",
    "- max_iters = 3000\n",
    "- gamma = 0.0000025\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -1120068718.9752452\n",
    "\n",
    "Test error: -124086547.00130773\n",
    "\n",
    "#### experiment 2\n",
    "Binarize -999 by -1 instead of zero\n",
    "- max_iters = 3000\n",
    "- gamma = 0.0000025\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -1120068718.9752452\n",
    "\n",
    "Test error: -124086547.00130773\n",
    "\n",
    "Does not seem to change anything to the computation\n",
    "\n",
    "#### experiment 3\n",
    "Remove standardization from course and 0 to 1 binarization for -999 values\n",
    "- max_iters = 3000\n",
    "- gamma = 0.00001\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: 36759800214.95009\n",
    "\n",
    "Test error: 4107345854.1234426\n",
    "\n",
    "#### experiment 4\n",
    "-1 to 1 binarization for -999 values\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -8683001618.57466\n",
    "\n",
    "Test error: -964334808.2575201\n",
    "\n",
    "This seems to improve the classification quite significantly\n",
    "\n",
    "#### experiment 5\n",
    "-1 to 1 for discrete features\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -10516946495.735361\n",
    "\n",
    "Test error: -1168079071.61974\n",
    "\n",
    "This also improves the test error by quite a margin\n",
    "\n",
    "#### experiment 6\n",
    "Remove feature scaling from exponential family distributions\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: 430048088.1856251\n",
    "\n",
    "Test error: 48278445.35947897\n",
    "\n",
    "This destroys the performance of the classifier\n",
    "\n",
    "\n",
    "#### experiment 7\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 250\n",
    "\n",
    "Training error: -10840473622.745983\n",
    "\n",
    "Test error: -1203987023.1683881\n",
    "\n",
    "Still an improvement over experiment 5, should explore setting the lambda to even lower values\n",
    "\n",
    "#### experiment 8\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 50\n",
    "\n",
    "Training error: -11435642570.154247\n",
    "\n",
    "Test error: -1270057446.4032438\n",
    "\n",
    "#### experiment 9\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 1\n",
    "\n",
    "Training error: -11267049837.002642\n",
    "\n",
    "Test error: -1251301717.207404\n",
    "\n",
    "Did not improve, the best lambda is probably between 50 and 1\n",
    "\n",
    "## experiment 10 BEST\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 25\n",
    "\n",
    "Training error: -11452909733.500443\n",
    "\n",
    "Test error: -1271963954.567976\n",
    "\n",
    "Improved, should test lambda=32 and 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_index = mean_loss_te.index(min(mean_loss_te))\n",
    "lambda_star = lambdas[min_index]\n",
    "w = reg_logistic_regression(y, tX, 0, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_te = preprocess_inputs(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = sigmoid(tX_te @ w)\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
