{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.seterr(over='raise');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "# preprocess data\n",
    "tX = preprocess_inputs(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=129965.09635498974\n",
      "Current iteration=100, the loss=-10321121686.590923\n",
      "Current iteration=200, the loss=-10662185557.344978\n",
      "Current iteration=300, the loss=-10899801333.320663\n",
      "Current iteration=400, the loss=-11071298446.424082\n",
      "Current iteration=500, the loss=-11196904827.619644\n",
      "Current iteration=600, the loss=-11289929649.186924\n",
      "Current iteration=700, the loss=-11359493394.944494\n",
      "Current iteration=800, the loss=-11411996894.58586\n",
      "Current iteration=900, the loss=-11452009454.978436\n",
      "Current iteration=1000, the loss=-11482832185.000849\n",
      "Current iteration=1100, the loss=-11506869584.279968\n",
      "Current iteration=1200, the loss=-11525882702.74843\n",
      "Current iteration=1300, the loss=-11541165982.769478\n",
      "Current iteration=1400, the loss=-11553673089.767635\n",
      "Current iteration=1500, the loss=-11564107629.622704\n",
      "Current iteration=1600, the loss=-11572989149.197577\n",
      "Current iteration=1700, the loss=-11580701451.170324\n",
      "Current iteration=1800, the loss=-11587528106.962088\n",
      "Current iteration=1900, the loss=-11593678629.01548\n",
      "Current iteration=2000, the loss=-11599307791.584774\n",
      "Current iteration=2100, the loss=-11604529908.486986\n",
      "Current iteration=2200, the loss=-11609429390.782408\n",
      "Current iteration=2300, the loss=-11614068556.583757\n",
      "Current iteration=2400, the loss=-11618493409.448572\n",
      "Current iteration=2500, the loss=-11622737914.251244\n",
      "Current iteration=2600, the loss=-11626827161.357763\n",
      "Current iteration=2700, the loss=-11630779708.076883\n",
      "Current iteration=2800, the loss=-11634609311.09062\n",
      "Current iteration=2900, the loss=-11638326207.96805\n",
      "The loss=-11641940108.802439\n",
      "Current iteration=0, the loss=129965.09635498974\n",
      "Current iteration=100, the loss=-10027495115.822893\n",
      "Current iteration=200, the loss=-10361038727.198708\n",
      "Current iteration=300, the loss=-10594377743.501017\n",
      "Current iteration=400, the loss=-10763623838.322086\n",
      "Current iteration=500, the loss=-10888262884.057018\n",
      "Current iteration=600, the loss=-10981150126.79707\n",
      "Current iteration=700, the loss=-11051114107.770948\n",
      "Current iteration=800, the loss=-11104358500.429651\n",
      "Current iteration=900, the loss=-11145315708.991217\n",
      "Current iteration=1000, the loss=-11177190962.115725\n",
      "Current iteration=1100, the loss=-11202321990.418655\n",
      "Current iteration=1200, the loss=-11222424143.406555\n",
      "Current iteration=1300, the loss=-11238761497.621185\n",
      "Current iteration=1400, the loss=-11252268480.716587\n",
      "Current iteration=1500, the loss=-11263637465.189648\n",
      "Current iteration=1600, the loss=-11273382443.472525\n",
      "Current iteration=1700, the loss=-11281885616.73815\n",
      "Current iteration=1800, the loss=-11289431634.838106\n",
      "Current iteration=1900, the loss=-11296232837.943066\n",
      "Current iteration=2000, the loss=-11302447904.41183\n",
      "Current iteration=2100, the loss=-11308195648.516436\n",
      "Current iteration=2200, the loss=-11313565241.474916\n",
      "Current iteration=2300, the loss=-11318623790.357908\n",
      "Current iteration=2400, the loss=-11323421962.884212\n",
      "Current iteration=2500, the loss=-11327998165.649635\n",
      "Current iteration=2600, the loss=-11332381650.682732\n",
      "Current iteration=2700, the loss=-11336594827.482555\n",
      "Current iteration=2800, the loss=-11340654985.555693\n",
      "Current iteration=2900, the loss=-11344575579.19529\n",
      "The loss=-11348369199.365036\n",
      "Current iteration=0, the loss=129965.09635498974\n",
      "Current iteration=100, the loss=-10118487403.228434\n",
      "Current iteration=200, the loss=-10455576079.141296\n",
      "Current iteration=300, the loss=-10691552544.79592\n",
      "Current iteration=400, the loss=-10862788546.980356\n",
      "Current iteration=500, the loss=-10989013363.68983\n",
      "Current iteration=600, the loss=-11083214801.019203\n",
      "Current iteration=700, the loss=-11154296685.844612\n",
      "Current iteration=800, the loss=-11208510253.61822\n",
      "Current iteration=900, the loss=-11250320451.731392\n",
      "Current iteration=1000, the loss=-11282955905.550356\n",
      "Current iteration=1100, the loss=-11308771703.804441\n",
      "Current iteration=1200, the loss=-11329496368.809351\n",
      "Current iteration=1300, the loss=-11346404159.606691\n",
      "Current iteration=1400, the loss=-11360437501.021132\n",
      "Current iteration=1500, the loss=-11372295133.26811\n",
      "Current iteration=1600, the loss=-11382496177.814596\n",
      "Current iteration=1700, the loss=-11391427005.765642\n",
      "Current iteration=1800, the loss=-11399375681.934439\n",
      "Current iteration=1900, the loss=-11406557358.985367\n",
      "Current iteration=2000, the loss=-11413133042.032213\n",
      "Current iteration=2100, the loss=-11419223477.771015\n",
      "Current iteration=2200, the loss=-11424919448.520647\n",
      "Current iteration=2300, the loss=-11430289410.31375\n",
      "Current iteration=2400, the loss=-11435385166.144032\n",
      "Current iteration=2500, the loss=-11440246084.078068\n",
      "Current iteration=2600, the loss=-11444902236.67919\n",
      "Current iteration=2700, the loss=-11449376740.09286\n",
      "Current iteration=2800, the loss=-11453687498.75413\n",
      "Current iteration=2900, the loss=-11457848508.244555\n",
      "The loss=-11461872845.662851\n",
      "Current iteration=0, the loss=129965.09635498974\n",
      "Current iteration=100, the loss=-10081027357.821972\n",
      "Current iteration=200, the loss=-10410046857.737776\n",
      "Current iteration=300, the loss=-10638911600.47446\n",
      "Current iteration=400, the loss=-10803923201.121912\n",
      "Current iteration=500, the loss=-10924700139.384521\n",
      "Current iteration=600, the loss=-11014136172.45942\n",
      "Current iteration=700, the loss=-11081054565.178898\n",
      "Current iteration=800, the loss=-11131633961.024866\n",
      "Current iteration=900, the loss=-11170273122.73675\n",
      "Current iteration=1000, the loss=-11200140700.1236\n",
      "Current iteration=1100, the loss=-11223538040.095991\n",
      "Current iteration=1200, the loss=-11242146064.744465\n",
      "Current iteration=1300, the loss=-11257197305.666622\n",
      "Current iteration=1400, the loss=-11269597920.242903\n",
      "Current iteration=1500, the loss=-11280015339.927744\n",
      "Current iteration=1600, the loss=-11288941797.566273\n",
      "Current iteration=1700, the loss=-11296740658.795155\n",
      "Current iteration=1800, the loss=-11303680356.303288\n",
      "Current iteration=1900, the loss=-11309959316.146416\n",
      "Current iteration=2000, the loss=-11315724303.338749\n",
      "Current iteration=2100, the loss=-11321083942.330723\n",
      "Current iteration=2200, the loss=-11326118690.926281\n",
      "Current iteration=2300, the loss=-11330888203.10851\n",
      "Current iteration=2400, the loss=-11335436767.316307\n",
      "Current iteration=2500, the loss=-11339797325.049692\n",
      "Current iteration=2600, the loss=-11343994441.568853\n",
      "Current iteration=2700, the loss=-11348046502.67233\n",
      "Current iteration=2800, the loss=-11351967339.604374\n",
      "Current iteration=2900, the loss=-11355767431.169626\n",
      "The loss=-11359456780.171446\n",
      "Training error: -11452909733.500443\n",
      "Test error: -1271963954.567976\n"
     ]
    }
   ],
   "source": [
    "from toolbox import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 3000\n",
    "gamma = 0.000005\n",
    "lambda_ = 25\n",
    "batch_size = 100\n",
    "k = 4\n",
    "k_indices = build_k_indices(y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "\n",
    "for k_ in range(k):\n",
    "    \n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (y[test_indices], tX[test_indices])\n",
    "    \n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (y[training_indices], tX[training_indices])\n",
    "    \n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_, gamma, max_iters)\n",
    "    \n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result analysis\n",
    "\n",
    "#### experiment 1\n",
    "- max_iters = 3000\n",
    "- gamma = 0.0000025\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -1120068718.9752452\n",
    "\n",
    "Test error: -124086547.00130773\n",
    "\n",
    "#### experiment 2\n",
    "Binarize -999 by -1 instead of zero\n",
    "- max_iters = 3000\n",
    "- gamma = 0.0000025\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -1120068718.9752452\n",
    "\n",
    "Test error: -124086547.00130773\n",
    "\n",
    "Does not seem to change anything to the computation\n",
    "\n",
    "#### experiment 3\n",
    "Remove standardization from course and 0 to 1 binarization for -999 values\n",
    "- max_iters = 3000\n",
    "- gamma = 0.00001\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: 36759800214.95009\n",
    "\n",
    "Test error: 4107345854.1234426\n",
    "\n",
    "#### experiment 4\n",
    "-1 to 1 binarization for -999 values\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -8683001618.57466\n",
    "\n",
    "Test error: -964334808.2575201\n",
    "\n",
    "This seems to improve the classification quite significantly\n",
    "\n",
    "#### experiment 5\n",
    "-1 to 1 for discrete features\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: -10516946495.735361\n",
    "\n",
    "Test error: -1168079071.61974\n",
    "\n",
    "This also improves the test error by quite a margin\n",
    "\n",
    "#### experiment 6\n",
    "Remove feature scaling from exponential family distributions\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 500\n",
    "\n",
    "Training error: 430048088.1856251\n",
    "\n",
    "Test error: 48278445.35947897\n",
    "\n",
    "This destroys the performance of the classifier\n",
    "\n",
    "\n",
    "#### experiment 7\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 250\n",
    "\n",
    "Training error: -10840473622.745983\n",
    "\n",
    "Test error: -1203987023.1683881\n",
    "\n",
    "Still an improvement over experiment 5, should explore setting the lambda to even lower values\n",
    "\n",
    "#### experiment 8\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 50\n",
    "\n",
    "Training error: -11435642570.154247\n",
    "\n",
    "Test error: -1270057446.4032438\n",
    "\n",
    "#### experiment 9\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 1\n",
    "\n",
    "Training error: -11267049837.002642\n",
    "\n",
    "Test error: -1251301717.207404\n",
    "\n",
    "Did not improve, the best lambda is probably between 50 and 1\n",
    "\n",
    "## experiment 10 BEST\n",
    "lambda modification\n",
    "- max_iters = 3000\n",
    "- gamma = 0.000005\n",
    "- lambda_ = 25\n",
    "\n",
    "Training error: -11452909733.500443\n",
    "\n",
    "Test error: -1271963954.567976\n",
    "\n",
    "Improved, should test lambda=32 and 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1066070423.2348557"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if -1271963954.567976 < -1270057446.4032438:\n",
    "    print(\"YES\")\n",
    "203987023.1683881-1270057446.4032438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=173286.79513998624\n",
      "Current iteration=1000, the loss=-23667471819.96783\n",
      "Current iteration=2000, the loss=-23738787341.081955\n",
      "The loss=-23741795523.01027\n"
     ]
    }
   ],
   "source": [
    "w = reg_logistic_regression(y, tX, lambda_, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_te = preprocess_inputs(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = sigmoid(tX_te @ w)\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358878,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred > 0.5].shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
