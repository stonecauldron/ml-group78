{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.seterr(over='raise', invalid='raise', divide='raise');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "X0, X1, X2, X3, indices0, indices1, indices2, indices3 = separate_data(X)\n",
    "y0 = y[indices0]\n",
    "y1 = y[indices1]\n",
    "y2 = y[indices2]\n",
    "y3 = y[indices3]\n",
    "\n",
    "overall_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 29 is out of bounds for axis 1 with size 29",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-c2a879ee9d7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: index 29 is out of bounds for axis 1 with size 29"
     ]
    }
   ],
   "source": [
    "X2[0,29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=51077.75502434178\n",
      "Current iteration=100, the loss=35217.841795553366\n",
      "Current iteration=200, the loss=32609.4501322801\n",
      "Current iteration=300, the loss=31382.362784139026\n",
      "Current iteration=400, the loss=30684.31288022869\n",
      "Current iteration=500, the loss=30246.201695432283\n",
      "Current iteration=600, the loss=29952.215172550437\n",
      "Current iteration=700, the loss=29744.320884997996\n",
      "Current iteration=800, the loss=29590.663796521523\n",
      "Current iteration=900, the loss=29472.633446580076\n",
      "Current iteration=1000, the loss=29378.841933975345\n",
      "Current iteration=1100, the loss=29302.06056912399\n",
      "Current iteration=1200, the loss=29237.55821836039\n",
      "Current iteration=1300, the loss=29182.154688918607\n",
      "Current iteration=1400, the loss=29133.660093086535\n",
      "Current iteration=1500, the loss=29090.53216601306\n",
      "Current iteration=1600, the loss=29051.661259101384\n",
      "Current iteration=1700, the loss=29016.232417424566\n",
      "Current iteration=1800, the loss=28983.635174427756\n",
      "Current iteration=1900, the loss=28953.403510535234\n",
      "Current iteration=2000, the loss=28925.175226883697\n",
      "Current iteration=2100, the loss=28898.664019763786\n",
      "Current iteration=2200, the loss=28873.639989999996\n",
      "Current iteration=2300, the loss=28849.915835872616\n",
      "Current iteration=2400, the loss=28827.336929524696\n",
      "Current iteration=2500, the loss=28805.77408291289\n",
      "Current iteration=2600, the loss=28785.118200774406\n",
      "Current iteration=2700, the loss=28765.276274227588\n",
      "Current iteration=2800, the loss=28746.168338440963\n",
      "Current iteration=2900, the loss=28727.7251318199\n",
      "Current iteration=3000, the loss=28709.886271628056\n",
      "Current iteration=3100, the loss=28692.598814196084\n",
      "Current iteration=3200, the loss=28675.81610483976\n",
      "Current iteration=3300, the loss=28659.49684853672\n",
      "Current iteration=3400, the loss=28643.60435076345\n",
      "Current iteration=3500, the loss=28628.105890997533\n",
      "Current iteration=3600, the loss=28612.97220082595\n",
      "Current iteration=3700, the loss=28598.177025450557\n",
      "Current iteration=3800, the loss=28583.696752395095\n",
      "Current iteration=3900, the loss=28569.510094919846\n",
      "Current iteration=4000, the loss=28555.597820402934\n",
      "Current iteration=4100, the loss=28541.94251601583\n",
      "Current iteration=4200, the loss=28528.528385587026\n",
      "Current iteration=4300, the loss=28515.341072744857\n",
      "Current iteration=4400, the loss=28502.367506355637\n",
      "Current iteration=4500, the loss=28489.595764993144\n",
      "Current iteration=4600, the loss=28477.01495774246\n",
      "Current iteration=4700, the loss=28464.615119091726\n",
      "Current iteration=4800, the loss=28452.387116026657\n",
      "Current iteration=4900, the loss=28440.322565734943\n",
      "Current iteration=5000, the loss=28428.41376256651\n",
      "Current iteration=5100, the loss=28416.653613092683\n",
      "Current iteration=5200, the loss=28405.035578269548\n",
      "Current iteration=5300, the loss=28393.55362184797\n",
      "Current iteration=5400, the loss=28382.20216428715\n",
      "Current iteration=5500, the loss=28370.976041524973\n",
      "Current iteration=5600, the loss=28359.870468041878\n",
      "Current iteration=5700, the loss=28348.88100372447\n",
      "Current iteration=5800, the loss=28338.003524096333\n",
      "Current iteration=5900, the loss=28327.234193534932\n",
      "Current iteration=6000, the loss=28316.569441138978\n",
      "Current iteration=6100, the loss=28306.005938949816\n",
      "Current iteration=6200, the loss=28295.540582264148\n",
      "Current iteration=6300, the loss=28285.170471805675\n",
      "Current iteration=6400, the loss=28274.89289754955\n",
      "Current iteration=6500, the loss=28264.705324016508\n",
      "Current iteration=6600, the loss=28254.605376874693\n",
      "Current iteration=6700, the loss=28244.590830705532\n",
      "Current iteration=6800, the loss=28234.6595978068\n",
      "Current iteration=6900, the loss=28224.809717921198\n",
      "Current iteration=7000, the loss=28215.039348792216\n",
      "Current iteration=7100, the loss=28205.346757462088\n",
      "Current iteration=7200, the loss=28195.730312237458\n",
      "Current iteration=7300, the loss=28186.188475258554\n",
      "Current iteration=7400, the loss=28176.719795617464\n",
      "Current iteration=7500, the loss=28167.322902978427\n",
      "Current iteration=7600, the loss=28157.996501660375\n",
      "Current iteration=7700, the loss=28148.739365148147\n",
      "Current iteration=7800, the loss=28139.550331003265\n",
      "Current iteration=7900, the loss=28130.428296149374\n",
      "Current iteration=8000, the loss=28121.37221251047\n",
      "Current iteration=8100, the loss=28112.381082982065\n",
      "Current iteration=8200, the loss=28103.453957716934\n",
      "Current iteration=8300, the loss=28094.589930708076\n",
      "Current iteration=8400, the loss=28085.78813665179\n",
      "Current iteration=8500, the loss=28077.047748073855\n",
      "Current iteration=8600, the loss=28068.367972701828\n",
      "Current iteration=8700, the loss=28059.748051066235\n",
      "Current iteration=8800, the loss=28051.187254312834\n",
      "Current iteration=8900, the loss=28042.684882208603\n",
      "Current iteration=9000, the loss=28034.240261323364\n",
      "Current iteration=9100, the loss=28025.8527433695\n",
      "Current iteration=9200, the loss=28017.521703682178\n",
      "Current iteration=9300, the loss=28009.24653982294\n",
      "Current iteration=9400, the loss=28001.02667029017\n",
      "Current iteration=9500, the loss=27992.861533320694\n",
      "Current iteration=9600, the loss=27984.75058576781\n",
      "Current iteration=9700, the loss=27976.693302042186\n",
      "Current iteration=9800, the loss=27968.689173103594\n",
      "Current iteration=9900, the loss=27960.737705493288\n",
      "Current iteration=10000, the loss=27952.838420398064\n",
      "Current iteration=10100, the loss=27944.990852739975\n",
      "Current iteration=10200, the loss=27937.19455028721\n",
      "Current iteration=10300, the loss=27929.449072783405\n",
      "Current iteration=10400, the loss=27921.75399109619\n",
      "Current iteration=10500, the loss=27914.1088863857\n",
      "Current iteration=10600, the loss=27906.513349297373\n",
      "Current iteration=10700, the loss=27898.966979183064\n",
      "Current iteration=10800, the loss=27891.469383356532\n",
      "Current iteration=10900, the loss=27884.020176389087\n",
      "Current iteration=11000, the loss=27876.61897945116\n",
      "Current iteration=11100, the loss=27869.265419705236\n",
      "Current iteration=11200, the loss=27861.959129754476\n",
      "Current iteration=11300, the loss=27854.69974714994\n",
      "Current iteration=11400, the loss=27847.486913958237\n",
      "Current iteration=11500, the loss=27840.3202763895\n",
      "Current iteration=11600, the loss=27833.199484484525\n",
      "Current iteration=11700, the loss=27826.124191858315\n",
      "Current iteration=11800, the loss=27819.094055496083\n",
      "Current iteration=11900, the loss=27812.1087355973\n",
      "Current iteration=12000, the loss=27805.16789546229\n",
      "Current iteration=12100, the loss=27798.27120141606\n",
      "Current iteration=12200, the loss=27791.41832276341\n",
      "Current iteration=12300, the loss=27784.608931770086\n",
      "Current iteration=12400, the loss=27777.842703664635\n",
      "Current iteration=12500, the loss=27771.119316656146\n",
      "Current iteration=12600, the loss=27764.438451963793\n",
      "Current iteration=12700, the loss=27757.79979385405\n",
      "Current iteration=12800, the loss=27751.20302968286\n",
      "Current iteration=12900, the loss=27744.64784993966\n",
      "The loss=27738.198883979272\n",
      "Current iteration=0, the loss=51071.30766174322\n",
      "Current iteration=100, the loss=35142.46245421559\n",
      "Current iteration=200, the loss=32510.085441212006\n",
      "Current iteration=300, the loss=31266.199402151266\n",
      "Current iteration=400, the loss=30558.27437330221\n",
      "Current iteration=500, the loss=30114.30529425904\n",
      "Current iteration=600, the loss=29816.119241467688\n",
      "Current iteration=700, the loss=29604.889705176432\n",
      "Current iteration=800, the loss=29448.51798154356\n",
      "Current iteration=900, the loss=29328.281054589737\n",
      "Current iteration=1000, the loss=29232.70980489523\n",
      "Current iteration=1100, the loss=29154.50403162406\n",
      "Current iteration=1200, the loss=29088.869522713343\n",
      "Current iteration=1300, the loss=29032.5736461126\n",
      "Current iteration=1400, the loss=28983.385420267492\n",
      "Current iteration=1500, the loss=28939.731735158377\n",
      "Current iteration=1600, the loss=28900.480363988558\n",
      "Current iteration=1700, the loss=28864.799942010766\n",
      "Current iteration=1800, the loss=28832.067935171854\n",
      "Current iteration=1900, the loss=28801.809171245444\n",
      "Current iteration=2000, the loss=28773.654176567285\n",
      "Current iteration=2100, the loss=28747.31054115321\n",
      "Current iteration=2200, the loss=28722.54296984692\n",
      "Current iteration=2300, the loss=28699.15919684346\n",
      "Current iteration=2400, the loss=28676.999905124503\n",
      "Current iteration=2500, the loss=28655.93141277591\n",
      "Current iteration=2600, the loss=28635.840292438916\n",
      "Current iteration=2700, the loss=28616.629356608166\n",
      "Current iteration=2800, the loss=28598.21461897374\n",
      "Current iteration=2900, the loss=28580.522961383147\n",
      "Current iteration=3000, the loss=28563.490317034808\n",
      "Current iteration=3100, the loss=28547.06023600055\n",
      "Current iteration=3200, the loss=28531.18273748611\n",
      "Current iteration=3300, the loss=28515.813379906256\n",
      "Current iteration=3400, the loss=28500.91249856543\n",
      "Current iteration=3500, the loss=28486.44457397571\n",
      "Current iteration=3600, the loss=28472.377703289123\n",
      "Current iteration=3700, the loss=28458.683154119306\n",
      "Current iteration=3800, the loss=28445.334984962672\n",
      "Current iteration=3900, the loss=28432.309720046575\n",
      "Current iteration=4000, the loss=28419.586069108987\n",
      "Current iteration=4100, the loss=28407.144684615876\n",
      "Current iteration=4200, the loss=28394.967950434897\n",
      "Current iteration=4300, the loss=28383.039797138423\n",
      "Current iteration=4400, the loss=28371.34554000175\n",
      "Current iteration=4500, the loss=28359.871736457448\n",
      "Current iteration=4600, the loss=28348.606060317034\n",
      "Current iteration=4700, the loss=28337.537190508847\n",
      "Current iteration=4800, the loss=28326.65471243408\n",
      "Current iteration=4900, the loss=28315.949030330397\n",
      "Current iteration=5000, the loss=28305.411289268188\n",
      "Current iteration=5100, the loss=28295.033305599507\n",
      "Current iteration=5200, the loss=28284.807504843193\n",
      "Current iteration=5300, the loss=28274.726866125908\n",
      "Current iteration=5400, the loss=28264.784872415014\n",
      "Current iteration=5500, the loss=28254.975465877367\n",
      "Current iteration=5600, the loss=28245.29300778223\n",
      "Current iteration=5700, the loss=28235.732242438986\n",
      "Current iteration=5800, the loss=28226.288264722076\n",
      "Current iteration=5900, the loss=28216.95649079005\n",
      "Current iteration=6000, the loss=28207.73263165182\n",
      "Current iteration=6100, the loss=28198.612669274204\n",
      "Current iteration=6200, the loss=28189.592834960247\n",
      "Current iteration=6300, the loss=28180.669589758654\n",
      "Current iteration=6400, the loss=28171.839606692392\n",
      "Current iteration=6500, the loss=28163.0997546175\n",
      "Current iteration=6600, the loss=28154.447083545314\n",
      "Current iteration=6700, the loss=28145.87881127849\n",
      "Current iteration=6800, the loss=28137.392311228745\n",
      "Current iteration=6900, the loss=28128.98510129771\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-93bd189d8b6d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtraining_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_cos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mw_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# compute classification accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bojan\\Desktop\\ml-group78.git\\scripts\\toolbox.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, gamma, max_iters)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# get loss and updated w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bojan\\Desktop\\ml-group78.git\\scripts\\toolbox.py\u001b[0m in \u001b[0;36mcalculate_gradient_log_likelihood\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;31m# stochastic gradient descent helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from toolbox import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters0 = 13000\n",
    "gamma0 = 1e-6\n",
    "lambda_0 = 0\n",
    "degree_0 = 2\n",
    "batch_size0 = 100\n",
    "k = 4\n",
    "current_y = y0\n",
    "current_X = X0\n",
    "k_indices = build_k_indices(current_y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (current_y[test_indices], current_X[test_indices])\n",
    "    test_x = build_poly_cos(test_x, degree_0, np.array(range(1,19)))\n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (current_y[training_indices], current_X[training_indices])\n",
    "    training_x = build_poly_cos(training_x, degree_0, np.array(range(1,19)))\n",
    "\n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_0, gamma0, max_iters0)\n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    test_y[test_y == 0] = -1\n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "    test_y[test_y == -1] = 0\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))\n",
    "overall_acc.append(np.mean(classification_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=39570.11790948118\n",
      "Current iteration=100, the loss=34009.767279587875\n",
      "Current iteration=200, the loss=32103.105836837378\n",
      "Current iteration=300, the loss=31248.475864745244\n",
      "Current iteration=400, the loss=30591.321859293206\n",
      "Current iteration=500, the loss=30013.460465126424\n",
      "Current iteration=600, the loss=29565.980190167975\n",
      "Current iteration=700, the loss=29215.53905482576\n",
      "Current iteration=800, the loss=28930.085801704678\n",
      "Current iteration=900, the loss=28691.25505826724\n",
      "Current iteration=1000, the loss=28488.103543151166\n",
      "Current iteration=1100, the loss=28313.00805625952\n",
      "Current iteration=1200, the loss=28160.357048396487\n",
      "Current iteration=1300, the loss=28025.922756879627\n",
      "Current iteration=1400, the loss=27906.477850239087\n",
      "Current iteration=1500, the loss=27799.521980695303\n",
      "Current iteration=1600, the loss=27703.07435443104\n",
      "Current iteration=1700, the loss=27615.52120972511\n",
      "Current iteration=1800, the loss=27535.51343640057\n",
      "Current iteration=1900, the loss=27461.906670711294\n",
      "Current iteration=2000, the loss=27393.733188125676\n",
      "Current iteration=2100, the loss=27330.207715988512\n",
      "Current iteration=2200, the loss=27270.806090620175\n",
      "Current iteration=2300, the loss=27215.441747534787\n",
      "Current iteration=2400, the loss=27164.454248070175\n",
      "Current iteration=2500, the loss=27117.972636879982\n",
      "Current iteration=2600, the loss=27075.45434584421\n",
      "Current iteration=2700, the loss=27036.18939816514\n",
      "Current iteration=2800, the loss=26999.9230476401\n",
      "Current iteration=2900, the loss=26966.792131887472\n",
      "Current iteration=3000, the loss=26936.6158051698\n",
      "Current iteration=3100, the loss=26908.76929939568\n",
      "Current iteration=3200, the loss=26882.712577631886\n",
      "Current iteration=3300, the loss=26858.152816314217\n",
      "Current iteration=3400, the loss=26834.93035388766\n",
      "Current iteration=3500, the loss=26812.932306719245\n",
      "Current iteration=3600, the loss=26792.06312520394\n",
      "Current iteration=3700, the loss=26772.237326306295\n",
      "Current iteration=3800, the loss=26753.377547925913\n",
      "Current iteration=3900, the loss=26735.41358423529\n",
      "Current iteration=4000, the loss=26718.281619862275\n",
      "Current iteration=4100, the loss=26701.92355391132\n",
      "Current iteration=4200, the loss=26686.286398112697\n",
      "Current iteration=4300, the loss=26671.3217422967\n",
      "Current iteration=4400, the loss=26656.985280640703\n",
      "Current iteration=4500, the loss=26643.236392221177\n",
      "Current iteration=4600, the loss=26630.037769780938\n",
      "Current iteration=4700, the loss=26617.35509113099\n",
      "Current iteration=4800, the loss=26605.156728138754\n",
      "Current iteration=4900, the loss=26593.413488769795\n",
      "Current iteration=5000, the loss=26582.098388135022\n",
      "Current iteration=5100, the loss=26571.186444951432\n",
      "Current iteration=5200, the loss=26560.65450025085\n",
      "Current iteration=5300, the loss=26550.48105556565\n",
      "Current iteration=5400, the loss=26540.64612818144\n",
      "Current iteration=5500, the loss=26531.131121369937\n",
      "Current iteration=5600, the loss=26521.91870779886\n",
      "Current iteration=5700, the loss=26512.99272456329\n",
      "Current iteration=5800, the loss=26504.338078492736\n",
      "Current iteration=5900, the loss=26495.940660569107\n",
      "Current iteration=6000, the loss=26487.78726844161\n",
      "Current iteration=6100, the loss=26479.865536154848\n",
      "Current iteration=6200, the loss=26472.163870315897\n",
      "Current iteration=6300, the loss=26464.671392019925\n",
      "Current iteration=6400, the loss=26457.37788393566\n",
      "Current iteration=6500, the loss=26450.27374202111\n",
      "Current iteration=6600, the loss=26443.349931401674\n",
      "Current iteration=6700, the loss=26436.597945994938\n",
      "Current iteration=6800, the loss=26430.009771514175\n",
      "Current iteration=6900, the loss=26423.57785152233\n",
      "Current iteration=7000, the loss=26417.295056244755\n",
      "Current iteration=7100, the loss=26411.154653880643\n",
      "Current iteration=7200, the loss=26405.15028418063\n",
      "Current iteration=7300, the loss=26399.275934082987\n",
      "Current iteration=7400, the loss=26393.52591522251\n",
      "Current iteration=7500, the loss=26387.894843145135\n",
      "Current iteration=7600, the loss=26382.37761807874\n",
      "Current iteration=7700, the loss=26376.96940712541\n",
      "Current iteration=7800, the loss=26371.665627753828\n",
      "Current iteration=7900, the loss=26366.461932482227\n",
      "Current iteration=8000, the loss=26361.354194653253\n",
      "Current iteration=8100, the loss=26356.33849521109\n",
      "Current iteration=8200, the loss=26351.411110399575\n",
      "Current iteration=8300, the loss=26346.568500308083\n",
      "Current iteration=8400, the loss=26341.80729819785\n",
      "Current iteration=8500, the loss=26337.124300548065\n",
      "Current iteration=8600, the loss=26332.51645776613\n",
      "Current iteration=8700, the loss=26327.98086551164\n",
      "Current iteration=8800, the loss=26323.514756587283\n",
      "Current iteration=8900, the loss=26319.115493355057\n",
      "Current iteration=9000, the loss=26314.78056063833\n",
      "Current iteration=9100, the loss=26310.507559074773\n",
      "Current iteration=9200, the loss=26306.294198887168\n",
      "Current iteration=9300, the loss=26302.138294042306\n",
      "Current iteration=9400, the loss=26298.037756770387\n",
      "Current iteration=9500, the loss=26293.990592419577\n",
      "Current iteration=9600, the loss=26289.99489462233\n",
      "Current iteration=9700, the loss=26286.04884075209\n",
      "Current iteration=9800, the loss=26282.150687650217\n",
      "Current iteration=9900, the loss=26278.298767605167\n",
      "Current iteration=10000, the loss=26274.49148456655\n",
      "Current iteration=10100, the loss=26270.727310578946\n",
      "Current iteration=10200, the loss=26267.00478242031\n",
      "Current iteration=10300, the loss=26263.32249843215\n",
      "Current iteration=10400, the loss=26259.679115528554\n",
      "Current iteration=10500, the loss=26256.073346372887\n",
      "Current iteration=10600, the loss=26252.503956711193\n",
      "Current iteration=10700, the loss=26248.969762852408\n",
      "Current iteration=10800, the loss=26245.469629286374\n",
      "Current iteration=10900, the loss=26242.00246643067\n",
      "Current iteration=11000, the loss=26238.567228498723\n",
      "Current iteration=11100, the loss=26235.16291148126\n",
      "Current iteration=11200, the loss=26231.788551234706\n",
      "Current iteration=11300, the loss=26228.44322166974\n",
      "Current iteration=11400, the loss=26225.126033034045\n",
      "Current iteration=11500, the loss=26221.836130283802\n",
      "Current iteration=11600, the loss=26218.572691538484\n",
      "Current iteration=11700, the loss=26215.33492661431\n",
      "Current iteration=11800, the loss=26212.12207563155\n",
      "Current iteration=11900, the loss=26208.9334076916\n",
      "Current iteration=12000, the loss=26205.768219619767\n",
      "Current iteration=12100, the loss=26202.625834770093\n",
      "Current iteration=12200, the loss=26199.50560188862\n",
      "Current iteration=12300, the loss=26196.40689403192\n",
      "Current iteration=12400, the loss=26193.329107537837\n",
      "Current iteration=12500, the loss=26190.27166104551\n",
      "Current iteration=12600, the loss=26187.23399456212\n",
      "Current iteration=12700, the loss=26184.215568573472\n",
      "Current iteration=12800, the loss=26181.215863196638\n",
      "Current iteration=12900, the loss=26178.234377371893\n",
      "Current iteration=13000, the loss=26175.27062809212\n",
      "Current iteration=13100, the loss=26172.324149667755\n",
      "Current iteration=13200, the loss=26169.394493025084\n",
      "Current iteration=13300, the loss=26166.481225036747\n",
      "Current iteration=13400, the loss=26163.583927882177\n",
      "Current iteration=13500, the loss=26160.702198436964\n",
      "Current iteration=13600, the loss=26157.83564768917\n",
      "Current iteration=13700, the loss=26154.9839001818\n",
      "Current iteration=13800, the loss=26152.146593479734\n",
      "Current iteration=13900, the loss=26149.323377659923\n",
      "Current iteration=14000, the loss=26146.513914823903\n",
      "Current iteration=14100, the loss=26143.7178786315\n",
      "Current iteration=14200, the loss=26140.93495385443\n",
      "Current iteration=14300, the loss=26138.16483594918\n",
      "Current iteration=14400, the loss=26135.40723064815\n",
      "Current iteration=14500, the loss=26132.6618535679\n",
      "Current iteration=14600, the loss=26129.928429834286\n",
      "Current iteration=14700, the loss=26127.20669372301\n",
      "Current iteration=14800, the loss=26124.496388315405\n",
      "Current iteration=14900, the loss=26121.79726516842\n",
      "Current iteration=15000, the loss=26119.10908399837\n",
      "Current iteration=15100, the loss=26116.43161237765\n",
      "Current iteration=15200, the loss=26113.76462544393\n",
      "Current iteration=15300, the loss=26111.107905621164\n",
      "Current iteration=15400, the loss=26108.46124235205\n",
      "Current iteration=15500, the loss=26105.824431841254\n",
      "Current iteration=15600, the loss=26103.197276809013\n",
      "Current iteration=15700, the loss=26100.579586254626\n",
      "Current iteration=15800, the loss=26097.971175229384\n",
      "Current iteration=15900, the loss=26095.37186461865\n",
      "Current iteration=16000, the loss=26092.781480932405\n",
      "Current iteration=16100, the loss=26090.1998561043\n",
      "Current iteration=16200, the loss=26087.626827298387\n",
      "Current iteration=16300, the loss=26085.06223672368\n",
      "Current iteration=16400, the loss=26082.505931455737\n",
      "Current iteration=16500, the loss=26079.957763265327\n",
      "Current iteration=16600, the loss=26077.417588453765\n",
      "Current iteration=16700, the loss=26074.885267694513\n",
      "Current iteration=16800, the loss=26072.36066588094\n",
      "Current iteration=16900, the loss=26069.843651979994\n",
      "Current iteration=17000, the loss=26067.33409889137\n",
      "Current iteration=17100, the loss=26064.83188331208\n",
      "Current iteration=17200, the loss=26062.336885606266\n",
      "Current iteration=17300, the loss=26059.848989679827\n",
      "Current iteration=17400, the loss=26057.36808285978\n",
      "Current iteration=17500, the loss=26054.894055778328\n",
      "Current iteration=17600, the loss=26052.426802261085\n",
      "Current iteration=17700, the loss=26049.966219219597\n",
      "Current iteration=17800, the loss=26047.512206547824\n",
      "Current iteration=17900, the loss=26045.064667022514\n",
      "Current iteration=18000, the loss=26042.623506207197\n",
      "Current iteration=18100, the loss=26040.188632359823\n",
      "Current iteration=18200, the loss=26037.7599563437\n",
      "Current iteration=18300, the loss=26035.33739154182\n",
      "Current iteration=18400, the loss=26032.9208537742\n",
      "Current iteration=18500, the loss=26030.510261218264\n",
      "Current iteration=18600, the loss=26028.10553433226\n",
      "Current iteration=18700, the loss=26025.7065957812\n",
      "Current iteration=18800, the loss=26023.31337036576\n",
      "Current iteration=18900, the loss=26020.92578495348\n",
      "Current iteration=19000, the loss=26018.543768412626\n",
      "Current iteration=19100, the loss=26016.167251548344\n",
      "Current iteration=19200, the loss=26013.796167041073\n",
      "Current iteration=19300, the loss=26011.430449387157\n",
      "Current iteration=19400, the loss=26009.070034841694\n",
      "Current iteration=19500, the loss=26006.71486136309\n",
      "Current iteration=19600, the loss=26004.36486855997\n",
      "Current iteration=19700, the loss=26002.019997639632\n",
      "Current iteration=19800, the loss=25999.68019135844\n",
      "Current iteration=19900, the loss=25997.345393974003\n",
      "Current iteration=20000, the loss=25995.01555119882\n",
      "Current iteration=20100, the loss=25992.69061015576\n",
      "Current iteration=20200, the loss=25990.370519334992\n",
      "Current iteration=20300, the loss=25988.055228552297\n",
      "Current iteration=20400, the loss=25985.74468890891\n",
      "Current iteration=20500, the loss=25983.438852752828\n",
      "Current iteration=20600, the loss=25981.13767364122\n",
      "Current iteration=20700, the loss=25978.8411063043\n",
      "Current iteration=20800, the loss=25976.54910661031\n",
      "Current iteration=20900, the loss=25974.26163153172\n",
      "Current iteration=21000, the loss=25971.978639112673\n",
      "Current iteration=21100, the loss=25969.700088437246\n",
      "Current iteration=21200, the loss=25967.425939599132\n",
      "Current iteration=21300, the loss=25965.156153672026\n",
      "Current iteration=21400, the loss=25962.89069268113\n",
      "Current iteration=21500, the loss=25960.629519575647\n",
      "Current iteration=21600, the loss=25958.3725982021\n",
      "Current iteration=21700, the loss=25956.119893278494\n",
      "Current iteration=21800, the loss=25953.87137036947\n",
      "Current iteration=21900, the loss=25951.626995862123\n",
      "Current iteration=22000, the loss=25949.386736942677\n",
      "Current iteration=22100, the loss=25947.150561573926\n",
      "Current iteration=22200, the loss=25944.918438473374\n",
      "Current iteration=22300, the loss=25942.6903370921\n",
      "Current iteration=22400, the loss=25940.466227594286\n",
      "Current iteration=22500, the loss=25938.246080837445\n",
      "Current iteration=22600, the loss=25936.029868353155\n",
      "Current iteration=22700, the loss=25933.817562328633\n",
      "Current iteration=22800, the loss=25931.609135588675\n",
      "Current iteration=22900, the loss=25929.40456157824\n",
      "Current iteration=23000, the loss=25927.20381434571\n",
      "Current iteration=23100, the loss=25925.006868526376\n",
      "Current iteration=23200, the loss=25922.813699326845\n",
      "Current iteration=23300, the loss=25920.62428250961\n",
      "Current iteration=23400, the loss=25918.438594378174\n",
      "Current iteration=23500, the loss=25916.25661176282\n",
      "Current iteration=23600, the loss=25914.07831200659\n",
      "Current iteration=23700, the loss=25911.903672951776\n",
      "Current iteration=23800, the loss=25909.732672926963\n",
      "Current iteration=23900, the loss=25907.5652907342\n",
      "Current iteration=24000, the loss=25905.40150563688\n",
      "Current iteration=24100, the loss=25903.241297347733\n",
      "Current iteration=24200, the loss=25901.08464601734\n",
      "Current iteration=24300, the loss=25898.931532222952\n",
      "Current iteration=24400, the loss=25896.78193695764\n",
      "Current iteration=24500, the loss=25894.635841619827\n",
      "Current iteration=24600, the loss=25892.49322800303\n",
      "Current iteration=24700, the loss=25890.354078286065\n",
      "Current iteration=24800, the loss=25888.218375023465\n",
      "Current iteration=24900, the loss=25886.086101136127\n",
      "The loss=25844.207677830214\n",
      "Current iteration=0, the loss=39531.99452750477\n",
      "Current iteration=100, the loss=33978.66980503194\n",
      "Current iteration=200, the loss=32357.495867007536\n",
      "Current iteration=300, the loss=31163.72657559073\n",
      "Current iteration=400, the loss=30450.59387510874\n",
      "Current iteration=500, the loss=29821.010277593094\n",
      "Current iteration=600, the loss=29419.79330642536\n",
      "Current iteration=700, the loss=29099.966030233358\n",
      "Current iteration=800, the loss=28815.938718769394\n",
      "Current iteration=900, the loss=28579.89807560997\n",
      "Current iteration=1000, the loss=28381.970248697093\n",
      "Current iteration=1100, the loss=28214.090625183122\n",
      "Current iteration=1200, the loss=28070.277033746224\n",
      "Current iteration=1300, the loss=27946.319090152778\n",
      "Current iteration=1400, the loss=27839.403868692105\n",
      "Current iteration=1500, the loss=27747.538048550596\n",
      "Current iteration=1600, the loss=27668.526422724764\n",
      "Current iteration=1700, the loss=27596.84449558607\n",
      "Current iteration=1800, the loss=27527.307420234647\n",
      "Current iteration=1900, the loss=27461.825009512846\n",
      "Current iteration=2000, the loss=27401.403810683496\n",
      "Current iteration=2100, the loss=27345.759069907668\n",
      "Current iteration=2200, the loss=27294.412916924368\n",
      "Current iteration=2300, the loss=27246.898728367407\n",
      "Current iteration=2400, the loss=27202.806501953033\n",
      "Current iteration=2500, the loss=27161.79054809213\n",
      "Current iteration=2600, the loss=27123.558401978447\n",
      "Current iteration=2700, the loss=27087.85557250268\n",
      "Current iteration=2800, the loss=27054.455649600204\n",
      "Current iteration=2900, the loss=27023.15550492343\n",
      "Current iteration=3000, the loss=26993.77268446965\n",
      "Current iteration=3100, the loss=26966.14348408347\n",
      "Current iteration=3200, the loss=26940.121261219945\n",
      "Current iteration=3300, the loss=26915.574687786153\n",
      "Current iteration=3400, the loss=26892.384402929965\n",
      "Current iteration=3500, the loss=26870.43379219556\n",
      "Current iteration=3600, the loss=26849.608192959633\n",
      "Current iteration=3700, the loss=26829.824166214185\n",
      "Current iteration=3800, the loss=26811.01599218345\n",
      "Current iteration=3900, the loss=26793.115774170925\n",
      "Current iteration=4000, the loss=26776.05889505727\n",
      "Current iteration=4100, the loss=26759.786390645648\n",
      "Current iteration=4200, the loss=26744.244578899372\n",
      "Current iteration=4300, the loss=26729.38448029331\n",
      "Current iteration=4400, the loss=26715.161302402135\n",
      "Current iteration=4500, the loss=26701.53399027052\n",
      "Current iteration=4600, the loss=26688.46483448129\n",
      "Current iteration=4700, the loss=26675.919129188827\n",
      "Current iteration=4800, the loss=26663.864873116287\n",
      "Current iteration=4900, the loss=26652.272507359317\n",
      "Current iteration=5000, the loss=26641.114684689128\n",
      "Current iteration=5100, the loss=26630.366065840164\n",
      "Current iteration=5200, the loss=26620.003138969827\n",
      "Current iteration=5300, the loss=26610.0040590813\n",
      "Current iteration=5400, the loss=26600.348504710615\n",
      "Current iteration=5500, the loss=26591.017549604843\n",
      "Current iteration=5600, the loss=26581.993547472244\n",
      "Current iteration=5700, the loss=26573.260028179393\n",
      "Current iteration=5800, the loss=26564.801604013017\n",
      "Current iteration=5900, the loss=26556.603884826967\n",
      "Current iteration=6000, the loss=26548.653401061878\n",
      "Current iteration=6100, the loss=26540.937533765624\n",
      "Current iteration=6200, the loss=26533.444450858955\n",
      "Current iteration=6300, the loss=26526.163048989616\n",
      "Current iteration=6400, the loss=26519.082900400463\n",
      "Current iteration=6500, the loss=26512.19420430778\n",
      "Current iteration=6600, the loss=26505.487742345875\n",
      "Current iteration=6700, the loss=26498.954837685043\n",
      "Current iteration=6800, the loss=26492.587317474798\n",
      "Current iteration=6900, the loss=26486.377478301918\n",
      "Current iteration=7000, the loss=26480.318054386786\n",
      "Current iteration=7100, the loss=26474.40218827022\n",
      "Current iteration=7200, the loss=26468.623403768688\n",
      "Current iteration=7300, the loss=26462.97558099825\n",
      "Current iteration=7400, the loss=26457.452933287626\n",
      "Current iteration=7500, the loss=26452.049985818154\n",
      "Current iteration=7600, the loss=26446.76155584415\n",
      "Current iteration=7700, the loss=26441.5827343613\n",
      "Current iteration=7800, the loss=26436.508869102814\n",
      "Current iteration=7900, the loss=26431.535548754626\n",
      "Current iteration=8000, the loss=26426.65858829054\n",
      "Current iteration=8100, the loss=26421.874015337416\n",
      "Current iteration=8200, the loss=26417.17805748863\n",
      "Current iteration=8300, the loss=26412.567130490737\n",
      "Current iteration=8400, the loss=26408.037827235792\n",
      "Current iteration=8500, the loss=26403.586907496818\n",
      "Current iteration=8600, the loss=26399.2112883495\n",
      "Current iteration=8700, the loss=26394.908035228487\n",
      "Current iteration=8800, the loss=26390.67435357035\n",
      "Current iteration=8900, the loss=26386.507580999685\n",
      "Current iteration=9000, the loss=26382.40518001841\n",
      "Current iteration=9100, the loss=26378.364731161422\n",
      "Current iteration=9200, the loss=26374.38392658499\n",
      "Current iteration=9300, the loss=26370.46056405669\n",
      "Current iteration=9400, the loss=26366.592541318554\n",
      "Current iteration=9500, the loss=26362.777850796974\n",
      "Current iteration=9600, the loss=26359.014574635137\n",
      "Current iteration=9700, the loss=26355.300880025992\n",
      "Current iteration=9800, the loss=26351.63501482455\n",
      "Current iteration=9900, the loss=26348.01530342111\n",
      "Current iteration=10000, the loss=26344.44014285731\n",
      "Current iteration=10100, the loss=26340.907999169223\n",
      "Current iteration=10200, the loss=26337.417403942014\n",
      "Current iteration=10300, the loss=26333.966951062503\n",
      "Current iteration=10400, the loss=26330.555293656696\n",
      "Current iteration=10500, the loss=26327.181141200188\n",
      "Current iteration=10600, the loss=26323.84325679057\n",
      "Current iteration=10700, the loss=26320.540454571288\n",
      "Current iteration=10800, the loss=26317.27159729751\n",
      "Current iteration=10900, the loss=26314.035594035202\n",
      "Current iteration=11000, the loss=26310.831397984948\n",
      "Current iteration=11100, the loss=26307.65800442299\n",
      "Current iteration=11200, the loss=26304.51444875211\n",
      "Current iteration=11300, the loss=26301.399804656132\n",
      "Current iteration=11400, the loss=26298.313182351172\n",
      "Current iteration=11500, the loss=26295.25372692839\n",
      "Current iteration=11600, the loss=26292.220616782615\n",
      "Current iteration=11700, the loss=26289.213062121744\n",
      "Current iteration=11800, the loss=26286.23030355235\n",
      "Current iteration=11900, the loss=26283.271610736887\n",
      "Current iteration=12000, the loss=26280.336281118583\n",
      "Current iteration=12100, the loss=26277.42363871002\n",
      "Current iteration=12200, the loss=26274.533032941854\n",
      "Current iteration=12300, the loss=26271.663837568216\n",
      "Current iteration=12400, the loss=26268.815449625832\n",
      "Current iteration=12500, the loss=26265.98728844351\n",
      "Current iteration=12600, the loss=26263.178794699706\n",
      "Current iteration=12700, the loss=26260.38942952508\n",
      "Current iteration=12800, the loss=26257.618673648016\n",
      "Current iteration=12900, the loss=26254.866026580334\n",
      "Current iteration=13000, the loss=26252.131005841562\n",
      "Current iteration=13100, the loss=26249.413146219224\n",
      "Current iteration=13200, the loss=26246.71199906367\n",
      "Current iteration=13300, the loss=26244.027131615294\n",
      "Current iteration=13400, the loss=26241.358126362677\n",
      "Current iteration=13500, the loss=26238.704580429974\n",
      "Current iteration=13600, the loss=26236.066104991958\n",
      "Current iteration=13700, the loss=26233.442324715605\n",
      "Current iteration=13800, the loss=26230.832877226472\n",
      "Current iteration=13900, the loss=26228.237412598883\n",
      "Current iteration=14000, the loss=26225.655592868752\n",
      "Current iteration=14100, the loss=26223.087091567642\n",
      "Current iteration=14200, the loss=26220.531593277436\n",
      "Current iteration=14300, the loss=26217.98879320421\n",
      "Current iteration=14400, the loss=26215.45839677056\n",
      "Current iteration=14500, the loss=26212.940119225535\n",
      "Current iteration=14600, the loss=26210.433685271157\n",
      "Current iteration=14700, the loss=26207.93882870495\n",
      "Current iteration=14800, the loss=26205.455292077462\n",
      "Current iteration=14900, the loss=26202.982826364336\n",
      "Current iteration=15000, the loss=26200.52119065204\n",
      "Current iteration=15100, the loss=26198.070151836757\n",
      "Current iteration=15200, the loss=26195.629484335637\n",
      "Current iteration=15300, the loss=26193.198969810168\n",
      "Current iteration=15400, the loss=26190.7783969007\n",
      "Current iteration=15500, the loss=26188.367560971943\n",
      "Current iteration=15600, the loss=26185.96626386881\n",
      "Current iteration=15700, the loss=26183.57431368212\n",
      "Current iteration=15800, the loss=26181.191524523667\n",
      "Current iteration=15900, the loss=26178.81771631049\n",
      "Current iteration=16000, the loss=26176.45271455762\n",
      "Current iteration=16100, the loss=26174.09635017915\n",
      "Current iteration=16200, the loss=26171.74845929708\n",
      "Current iteration=16300, the loss=26169.40888305782\n",
      "Current iteration=16400, the loss=26167.077467455827\n",
      "Current iteration=16500, the loss=26164.754063164157\n",
      "Current iteration=16600, the loss=26162.438525371694\n",
      "Current iteration=16700, the loss=26160.13071362647\n",
      "Current iteration=16800, the loss=26157.83049168536\n",
      "Current iteration=16900, the loss=26155.53772736933\n",
      "Current iteration=17000, the loss=26153.25229242431\n",
      "Current iteration=17100, the loss=26150.974062387435\n",
      "Current iteration=17200, the loss=26148.702916458315\n",
      "Current iteration=17300, the loss=26146.43873737526\n",
      "Current iteration=17400, the loss=26144.1814112961\n",
      "Current iteration=17500, the loss=26141.930827683675\n",
      "Current iteration=17600, the loss=26139.686879195422\n",
      "Current iteration=17700, the loss=26137.44946157723\n",
      "Current iteration=17800, the loss=26135.218473561345\n",
      "Current iteration=17900, the loss=26132.993816767834\n",
      "Current iteration=18000, the loss=26130.775395610046\n",
      "Current iteration=18100, the loss=26128.563117203284\n",
      "Current iteration=18200, the loss=26126.35689127706\n",
      "Current iteration=18300, the loss=26124.156630090532\n",
      "Current iteration=18400, the loss=26121.962248351065\n",
      "Current iteration=18500, the loss=26119.773663135737\n",
      "Current iteration=18600, the loss=26117.59079381587\n",
      "Current iteration=18700, the loss=26115.413561984213\n",
      "Current iteration=18800, the loss=26113.241891384827\n",
      "Current iteration=18900, the loss=26111.07570784558\n",
      "Current iteration=19000, the loss=26108.91493921305\n",
      "Current iteration=19100, the loss=26106.75951528988\n",
      "Current iteration=19200, the loss=26104.60936777435\n",
      "Current iteration=19300, the loss=26102.464430202264\n",
      "Current iteration=19400, the loss=26100.324637890808\n",
      "Current iteration=19500, the loss=26098.189927884607\n",
      "Current iteration=19600, the loss=26096.060238903705\n",
      "Current iteration=19700, the loss=26093.935511293403\n",
      "Current iteration=19800, the loss=26091.815686975988\n",
      "Current iteration=19900, the loss=26089.700709404242\n",
      "Current iteration=20000, the loss=26087.590523516636\n",
      "Current iteration=20100, the loss=26085.48507569413\n",
      "Current iteration=20200, the loss=26083.384313718656\n",
      "Current iteration=20300, the loss=26081.288186733094\n",
      "Current iteration=20400, the loss=26079.19664520273\n",
      "Current iteration=20500, the loss=26077.10964087813\n",
      "Current iteration=20600, the loss=26075.027126759458\n",
      "Current iteration=20700, the loss=26072.949057062084\n",
      "Current iteration=20800, the loss=26070.87538718349\n",
      "Current iteration=20900, the loss=26068.806073671487\n",
      "Current iteration=21000, the loss=26066.741074193567\n",
      "Current iteration=21100, the loss=26064.680347507463\n",
      "Current iteration=21200, the loss=26062.62385343286\n",
      "Current iteration=21300, the loss=26060.571552824193\n",
      "Current iteration=21400, the loss=26058.523407544482\n",
      "Current iteration=21500, the loss=26056.479380440243\n",
      "Current iteration=21600, the loss=26054.439435317392\n",
      "Current iteration=21700, the loss=26052.403536918027\n",
      "Current iteration=21800, the loss=26050.371650898338\n",
      "Current iteration=21900, the loss=26048.343743807123\n",
      "Current iteration=22000, the loss=26046.319783065548\n",
      "Current iteration=22100, the loss=26044.29973694743\n",
      "Current iteration=22200, the loss=26042.283574560515\n",
      "Current iteration=22300, the loss=26040.271265828498\n",
      "Current iteration=22400, the loss=26038.262781473786\n",
      "Current iteration=22500, the loss=26036.258093001074\n",
      "Current iteration=22600, the loss=26034.25717268147\n",
      "Current iteration=22700, the loss=26032.25999353746\n",
      "Current iteration=22800, the loss=26030.266529328408\n",
      "Current iteration=22900, the loss=26028.27675453671\n",
      "Current iteration=23000, the loss=26026.29064435455\n",
      "Current iteration=23100, the loss=26024.308174671103\n",
      "Current iteration=23200, the loss=26022.32932206036\n",
      "Current iteration=23300, the loss=26020.35406376937\n",
      "Current iteration=23400, the loss=26018.382377706865\n",
      "Current iteration=23500, the loss=26016.41424243236\n",
      "Current iteration=23600, the loss=26014.449637145495\n",
      "Current iteration=23700, the loss=26012.488541675648\n",
      "Current iteration=23800, the loss=26010.530936471914\n",
      "Current iteration=23900, the loss=26008.57680259303\n",
      "Current iteration=24000, the loss=26006.626121697518\n",
      "Current iteration=24100, the loss=26004.678876033846\n",
      "Current iteration=24200, the loss=26002.735048430466\n",
      "Current iteration=24300, the loss=26000.794622285714\n",
      "Current iteration=24400, the loss=25998.85758155748\n",
      "Current iteration=24500, the loss=25996.92391075261\n",
      "Current iteration=24600, the loss=25994.993594915675\n",
      "Current iteration=24700, the loss=25993.066619617333\n",
      "Current iteration=24800, the loss=25991.142970942026\n",
      "Current iteration=24900, the loss=25989.222635474776\n",
      "The loss=25938.858814316798\n",
      "Current iteration=0, the loss=39648.04933631299\n",
      "Current iteration=100, the loss=34047.92177218899\n",
      "Current iteration=200, the loss=32278.509893894345\n",
      "Current iteration=300, the loss=31173.430825270083\n",
      "Current iteration=400, the loss=30729.81819551025\n",
      "Current iteration=500, the loss=30170.59255654848\n",
      "Current iteration=600, the loss=29686.668184017755\n",
      "Current iteration=700, the loss=29281.174819962846\n",
      "Current iteration=800, the loss=29004.468211052976\n",
      "Current iteration=900, the loss=28827.544555176493\n",
      "Current iteration=1000, the loss=28636.517238203924\n",
      "Current iteration=1100, the loss=28462.374919392812\n",
      "Current iteration=1200, the loss=28307.086806489722\n",
      "Current iteration=1300, the loss=28167.33472545019\n",
      "Current iteration=1400, the loss=28039.972701649916\n",
      "Current iteration=1500, the loss=27930.02560854611\n",
      "Current iteration=1600, the loss=27841.394850520068\n",
      "Current iteration=1700, the loss=27763.082333601546\n",
      "Current iteration=1800, the loss=27691.455906528456\n",
      "Current iteration=1900, the loss=27620.0262760346\n",
      "Current iteration=2000, the loss=27565.19261013621\n",
      "Current iteration=2100, the loss=27518.252439905125\n",
      "Current iteration=2200, the loss=27461.678744462974\n",
      "Current iteration=2300, the loss=27400.544211763707\n",
      "Current iteration=2400, the loss=27346.77726794582\n",
      "Current iteration=2500, the loss=27311.168189153876\n",
      "Current iteration=2600, the loss=27285.19765355046\n",
      "Current iteration=2700, the loss=27266.30289149392\n",
      "Current iteration=2800, the loss=27208.354208864544\n",
      "Current iteration=2900, the loss=27194.722782832465\n",
      "Current iteration=3000, the loss=27155.836389464053\n",
      "Current iteration=3100, the loss=27140.42113622828\n",
      "Current iteration=3200, the loss=27112.992502247784\n",
      "Current iteration=3300, the loss=27087.680119404133\n",
      "Current iteration=3400, the loss=27071.737301357956\n",
      "Current iteration=3500, the loss=27021.345901313114\n",
      "Current iteration=3600, the loss=27027.051755072094\n",
      "Current iteration=3700, the loss=26997.835204300165\n",
      "Current iteration=3800, the loss=26990.960547906136\n",
      "Current iteration=3900, the loss=26958.917880296205\n",
      "Current iteration=4000, the loss=26956.235509180595\n",
      "Current iteration=4100, the loss=26927.345586338448\n",
      "Current iteration=4200, the loss=26913.978298597547\n",
      "Current iteration=4300, the loss=26901.995901539904\n",
      "Current iteration=4400, the loss=26870.769316919337\n",
      "Current iteration=4500, the loss=26883.838663137998\n",
      "Current iteration=4600, the loss=26856.47359456469\n",
      "Current iteration=4700, the loss=26852.27412661653\n",
      "Current iteration=4800, the loss=26836.678656205535\n",
      "Current iteration=4900, the loss=26816.137716833517\n",
      "Current iteration=5000, the loss=26813.370456790013\n",
      "Current iteration=5100, the loss=26807.723609380228\n",
      "Current iteration=5200, the loss=26785.279642657595\n",
      "Current iteration=5300, the loss=26791.887953527446\n",
      "Current iteration=5400, the loss=26754.27682836577\n",
      "Current iteration=5500, the loss=26762.465454968562\n",
      "Current iteration=5600, the loss=26765.305126174833\n",
      "Current iteration=5700, the loss=26727.258157147557\n",
      "Current iteration=5800, the loss=26730.34257537697\n",
      "Current iteration=5900, the loss=26734.304236770975\n",
      "Current iteration=6000, the loss=26723.214358053905\n",
      "Current iteration=6100, the loss=26693.77081224677\n",
      "Current iteration=6200, the loss=26692.167428631503\n",
      "Current iteration=6300, the loss=26696.98607964919\n",
      "Current iteration=6400, the loss=26696.753989844627\n",
      "Current iteration=6500, the loss=26689.048938711803\n",
      "Current iteration=6600, the loss=26664.847522072138\n",
      "Current iteration=6700, the loss=26650.292788503175\n",
      "Current iteration=6800, the loss=26643.18250258111\n",
      "Current iteration=6900, the loss=26643.04789662846\n",
      "Current iteration=7000, the loss=26643.85355469668\n",
      "Current iteration=7100, the loss=26642.728978410294\n",
      "Current iteration=7200, the loss=26640.158656533167\n",
      "Current iteration=7300, the loss=26636.478898926107\n",
      "Current iteration=7400, the loss=26631.012558800838\n",
      "Current iteration=7500, the loss=26622.602785941144\n",
      "Current iteration=7600, the loss=26612.537002326888\n",
      "Current iteration=7700, the loss=26602.83257538713\n",
      "Current iteration=7800, the loss=26593.975625221898\n",
      "Current iteration=7900, the loss=26585.864906942064\n",
      "Current iteration=8000, the loss=26578.386859280214\n",
      "Current iteration=8100, the loss=26571.487527968966\n",
      "Current iteration=8200, the loss=26565.142334430795\n",
      "Current iteration=8300, the loss=26559.326364045628\n",
      "Current iteration=8400, the loss=26553.99688747227\n",
      "Current iteration=8500, the loss=26549.087247361742\n",
      "Current iteration=8600, the loss=26544.51145953635\n",
      "Current iteration=8700, the loss=26540.177315662426\n",
      "Current iteration=8800, the loss=26536.002781881565\n",
      "Current iteration=8900, the loss=26531.929719906017\n",
      "Current iteration=9000, the loss=26527.931345999354\n",
      "Current iteration=9100, the loss=26524.012802917576\n",
      "Current iteration=9200, the loss=26520.20488540351\n",
      "Current iteration=9300, the loss=26516.548466604087\n",
      "Current iteration=9400, the loss=26513.06315430205\n",
      "Current iteration=9500, the loss=26509.69642805657\n",
      "Current iteration=9600, the loss=26506.28164055148\n",
      "Current iteration=9700, the loss=26502.594906097464\n",
      "Current iteration=9800, the loss=26498.54724787378\n",
      "Current iteration=9900, the loss=26494.289543985407\n",
      "Current iteration=10000, the loss=26490.036930174985\n",
      "Current iteration=10100, the loss=26485.870964370784\n",
      "Current iteration=10200, the loss=26481.769475430345\n",
      "Current iteration=10300, the loss=26477.708628333025\n",
      "Current iteration=10400, the loss=26473.684010970675\n",
      "Current iteration=10500, the loss=26469.695468759182\n",
      "Current iteration=10600, the loss=26465.74204844993\n",
      "Current iteration=10700, the loss=26461.82261493901\n",
      "Current iteration=10800, the loss=26457.93610303991\n",
      "Current iteration=10900, the loss=26454.08149616538\n",
      "Current iteration=11000, the loss=26450.25781989574\n",
      "Current iteration=11100, the loss=26446.46414050366\n",
      "Current iteration=11200, the loss=26442.699563109\n",
      "Current iteration=11300, the loss=26438.96322991183\n",
      "Current iteration=11400, the loss=26435.254318516214\n",
      "Current iteration=11500, the loss=26431.57204033597\n",
      "Current iteration=11600, the loss=26427.915639078867\n",
      "Current iteration=11700, the loss=26424.28438930481\n",
      "Current iteration=11800, the loss=26420.67759505487\n",
      "Current iteration=11900, the loss=26417.094588547574\n",
      "Current iteration=12000, the loss=26413.53472893927\n",
      "Current iteration=12100, the loss=26409.99740114594\n",
      "Current iteration=12200, the loss=26406.482014723308\n",
      "Current iteration=12300, the loss=26402.988002802853\n",
      "Current iteration=12400, the loss=26399.514821081317\n",
      "Current iteration=12500, the loss=26396.06194686117\n",
      "Current iteration=12600, the loss=26392.628878139767\n",
      "Current iteration=12700, the loss=26389.21513274487\n",
      "Current iteration=12800, the loss=26385.82024751463\n",
      "Current iteration=12900, the loss=26382.443777519256\n",
      "Current iteration=13000, the loss=26379.08529532273\n",
      "Current iteration=13100, the loss=26375.744390281976\n",
      "Current iteration=13200, the loss=26372.420667881404\n",
      "Current iteration=13300, the loss=26369.113749100754\n",
      "Current iteration=13400, the loss=26365.823269813904\n",
      "Current iteration=13500, the loss=26362.548880216455\n",
      "Current iteration=13600, the loss=26359.290244280448\n",
      "Current iteration=13700, the loss=26356.04703923361\n",
      "Current iteration=13800, the loss=26352.81895506164\n",
      "Current iteration=13900, the loss=26349.60569403165\n",
      "Current iteration=14000, the loss=26346.406970234977\n",
      "Current iteration=14100, the loss=26343.22250914783\n",
      "Current iteration=14200, the loss=26340.0520472083\n",
      "Current iteration=14300, the loss=26336.89533140851\n",
      "Current iteration=14400, the loss=26333.75211890058\n",
      "Current iteration=14500, the loss=26330.62217661518\n",
      "Current iteration=14600, the loss=26327.50528089198\n",
      "Current iteration=14700, the loss=26324.40121712065\n",
      "Current iteration=14800, the loss=26321.309779391882\n",
      "Current iteration=14900, the loss=26318.230770157305\n",
      "Current iteration=15000, the loss=26315.16399989766\n",
      "Current iteration=15100, the loss=26312.109286798448\n",
      "Current iteration=15200, the loss=26309.0664564321\n",
      "Current iteration=15300, the loss=26306.03534144616\n",
      "Current iteration=15400, the loss=26303.01578125652\n",
      "Current iteration=15500, the loss=26300.007621744757\n",
      "Current iteration=15600, the loss=26297.010714959208\n",
      "Current iteration=15700, the loss=26294.024918818453\n",
      "Current iteration=15800, the loss=26291.050096816623\n",
      "Current iteration=15900, the loss=26288.086117729577\n",
      "Current iteration=16000, the loss=26285.13285532109\n",
      "Current iteration=16100, the loss=26282.190188048233\n",
      "Current iteration=16200, the loss=26279.257998764802\n",
      "Current iteration=16300, the loss=26276.336174422486\n",
      "Current iteration=16400, the loss=26273.424605768305\n",
      "Current iteration=16500, the loss=26270.523187038027\n",
      "Current iteration=16600, the loss=26267.631815644698\n",
      "Current iteration=16700, the loss=26264.75039186165\n",
      "Current iteration=16800, the loss=26261.878818499543\n",
      "Current iteration=16900, the loss=26259.017000577034\n",
      "Current iteration=17000, the loss=26256.164844984854\n",
      "Current iteration=17100, the loss=26253.322260143283\n",
      "Current iteration=17200, the loss=26250.489155653253\n",
      "Current iteration=17300, the loss=26247.6654419414\n",
      "Current iteration=17400, the loss=26244.851029899917\n",
      "Current iteration=17500, the loss=26242.0458305221\n",
      "Current iteration=17600, the loss=26239.24975453507\n",
      "Current iteration=17700, the loss=26236.462712031236\n",
      "Current iteration=17800, the loss=26233.68461210073\n",
      "Current iteration=17900, the loss=26230.91536246726\n",
      "Current iteration=18000, the loss=26228.154869130274\n",
      "Current iteration=18100, the loss=26225.403036016825\n",
      "Current iteration=18200, the loss=26222.659764646563\n",
      "Current iteration=18300, the loss=26219.92495381445\n",
      "Current iteration=18400, the loss=26217.198499294933\n",
      "Current iteration=18500, the loss=26214.480293572742\n",
      "Current iteration=18600, the loss=26211.770225604698\n",
      "Current iteration=18700, the loss=26209.068180617855\n",
      "Current iteration=18800, the loss=26206.3740399483\n",
      "Current iteration=18900, the loss=26203.68768092583\n",
      "Current iteration=19000, the loss=26201.008976808276\n",
      "Current iteration=19100, the loss=26198.337796770054\n",
      "Current iteration=19200, the loss=26195.67400594744\n",
      "Current iteration=19300, the loss=26193.01746554362\n",
      "Current iteration=19400, the loss=26190.368032994884\n",
      "Current iteration=19500, the loss=26187.725562198295\n",
      "Current iteration=19600, the loss=26185.089903800457\n",
      "Current iteration=19700, the loss=26182.460905545275\n",
      "Current iteration=19800, the loss=26179.838412677644\n",
      "Current iteration=19900, the loss=26177.222268398713\n",
      "Current iteration=20000, the loss=26174.61231436714\n",
      "Current iteration=20100, the loss=26172.00839123953\n",
      "Current iteration=20200, the loss=26169.410339242735\n",
      "Current iteration=20300, the loss=26166.81799876934\n",
      "Current iteration=20400, the loss=26164.231210987626\n",
      "Current iteration=20500, the loss=26161.6498184568\n",
      "Current iteration=20600, the loss=26159.07366573802\n",
      "Current iteration=20700, the loss=26156.50259999239\n",
      "Current iteration=20800, the loss=26153.936471557117\n",
      "Current iteration=20900, the loss=26151.375134491922\n",
      "Current iteration=21000, the loss=26148.818447088415\n",
      "Current iteration=21100, the loss=26146.26627233649\n",
      "Current iteration=21200, the loss=26143.718478342555\n",
      "Current iteration=21300, the loss=26141.17493869573\n",
      "Current iteration=21400, the loss=26138.635532779444\n",
      "Current iteration=21500, the loss=26136.100146026867\n",
      "Current iteration=21600, the loss=26133.568670119897\n",
      "Current iteration=21700, the loss=26131.04100313251\n",
      "Current iteration=21800, the loss=26128.517049619877\n",
      "Current iteration=21900, the loss=26125.996720656127\n",
      "Current iteration=22000, the loss=26123.479933823575\n",
      "Current iteration=22100, the loss=26120.966613157256\n",
      "Current iteration=22200, the loss=26118.456689048995\n",
      "Current iteration=22300, the loss=26115.95009811521\n",
      "Current iteration=22400, the loss=26113.44678303285\n",
      "Current iteration=22500, the loss=26110.946692348403\n",
      "Current iteration=22600, the loss=26108.449780263858\n",
      "Current iteration=22700, the loss=26105.956006404384\n",
      "Current iteration=22800, the loss=26103.465335571556\n",
      "Current iteration=22900, the loss=26100.977737485857\n",
      "Current iteration=23000, the loss=26098.49318652218\n",
      "Current iteration=23100, the loss=26096.0116614412\n",
      "Current iteration=23200, the loss=26093.533145119687\n",
      "Current iteration=23300, the loss=26091.057624281922\n",
      "Current iteration=23400, the loss=26088.585089234744\n",
      "Current iteration=23500, the loss=26086.115533607623\n",
      "Current iteration=23600, the loss=26083.648954099495\n",
      "Current iteration=23700, the loss=26081.185350233674\n",
      "Current iteration=23800, the loss=26078.72472412149\n",
      "Current iteration=23900, the loss=26076.267080235713\n",
      "Current iteration=24000, the loss=26073.812425194163\n",
      "Current iteration=24100, the loss=26071.360767553757\n",
      "Current iteration=24200, the loss=26068.912117615415\n",
      "Current iteration=24300, the loss=26066.46648723976\n",
      "Current iteration=24400, the loss=26064.023889673415\n",
      "Current iteration=24500, the loss=26061.584339386154\n",
      "Current iteration=24600, the loss=26059.147851918293\n",
      "Current iteration=24700, the loss=26056.714443738234\n",
      "Current iteration=24800, the loss=26054.284132109846\n",
      "Current iteration=24900, the loss=26051.85693496932\n",
      "The loss=26002.96030440938\n",
      "Current iteration=0, the loss=39516.932795169494\n",
      "Current iteration=100, the loss=33764.31034994884\n",
      "Current iteration=200, the loss=32082.35726642437\n",
      "Current iteration=300, the loss=31044.15769273644\n",
      "Current iteration=400, the loss=30302.69439443686\n",
      "Current iteration=500, the loss=29746.410101333753\n",
      "Current iteration=600, the loss=29308.22931649217\n",
      "Current iteration=700, the loss=28959.080361189575\n",
      "Current iteration=800, the loss=28679.46122804181\n",
      "Current iteration=900, the loss=28447.862311307978\n",
      "Current iteration=1000, the loss=28252.13065320145\n",
      "Current iteration=1100, the loss=28084.039555561525\n",
      "Current iteration=1200, the loss=27938.081018042714\n",
      "Current iteration=1300, the loss=27810.145160651595\n",
      "Current iteration=1400, the loss=27697.09556289686\n",
      "Current iteration=1500, the loss=27596.492310290232\n",
      "Current iteration=1600, the loss=27506.403154935255\n",
      "Current iteration=1700, the loss=27425.263101610053\n",
      "Current iteration=1800, the loss=27351.77586462975\n",
      "Current iteration=1900, the loss=27284.88450672551\n",
      "Current iteration=2000, the loss=27223.725046936135\n",
      "Current iteration=2100, the loss=27167.584324819258\n",
      "Current iteration=2200, the loss=27115.8671762109\n",
      "Current iteration=2300, the loss=27068.07244681639\n",
      "Current iteration=2400, the loss=27023.782242125002\n",
      "Current iteration=2500, the loss=26982.686277111203\n",
      "Current iteration=2600, the loss=26944.50243281829\n",
      "Current iteration=2700, the loss=26908.882372916218\n",
      "Current iteration=2800, the loss=26875.592236199227\n",
      "Current iteration=2900, the loss=26844.42484002487\n",
      "Current iteration=3000, the loss=26815.18898296169\n",
      "Current iteration=3100, the loss=26787.71286078589\n",
      "Current iteration=3200, the loss=26761.843248000885\n",
      "Current iteration=3300, the loss=26737.443537862695\n",
      "Current iteration=3400, the loss=26714.39170144956\n",
      "Current iteration=3500, the loss=26692.578448046515\n",
      "Current iteration=3600, the loss=26671.905635893036\n",
      "Current iteration=3700, the loss=26652.284915518823\n",
      "Current iteration=3800, the loss=26633.636572974912\n",
      "Current iteration=3900, the loss=26615.888541423832\n",
      "Current iteration=4000, the loss=26598.97555506183\n",
      "Current iteration=4100, the loss=26582.83842553368\n",
      "Current iteration=4200, the loss=26567.423427248763\n",
      "Current iteration=4300, the loss=26552.681785144938\n",
      "Current iteration=4400, the loss=26538.569268542957\n",
      "Current iteration=4500, the loss=26525.045911992263\n",
      "Current iteration=4600, the loss=26512.075917052156\n",
      "Current iteration=4700, the loss=26499.627854744598\n",
      "Current iteration=4800, the loss=26487.675412845238\n",
      "Current iteration=4900, the loss=26476.199108373636\n",
      "Current iteration=5000, the loss=26465.18929657055\n",
      "Current iteration=5100, the loss=26454.64877585281\n",
      "Current iteration=5200, the loss=26444.585593571745\n",
      "Current iteration=5300, the loss=26434.98205988002\n",
      "Current iteration=5400, the loss=26425.770709036777\n",
      "Current iteration=5500, the loss=26416.87843314828\n",
      "Current iteration=5600, the loss=26408.26758516961\n",
      "Current iteration=5700, the loss=26399.919680546765\n",
      "Current iteration=5800, the loss=26391.820057719036\n",
      "Current iteration=5900, the loss=26383.95519396288\n",
      "Current iteration=6000, the loss=26376.312480442393\n",
      "Current iteration=6100, the loss=26368.880144439576\n",
      "Current iteration=6200, the loss=26361.64718227413\n",
      "Current iteration=6300, the loss=26354.603298930306\n",
      "Current iteration=6400, the loss=26347.738853439183\n",
      "Current iteration=6500, the loss=26341.04480930438\n",
      "Current iteration=6600, the loss=26334.512689390875\n",
      "Current iteration=6700, the loss=26328.134534788238\n",
      "Current iteration=6800, the loss=26321.902867229386\n",
      "Current iteration=6900, the loss=26315.810654701105\n",
      "Current iteration=7000, the loss=26309.85127992888\n",
      "Current iteration=7100, the loss=26304.018511456128\n",
      "Current iteration=7200, the loss=26298.30647707045\n",
      "Current iteration=7300, the loss=26292.709639357196\n",
      "Current iteration=7400, the loss=26287.22277318444\n",
      "Current iteration=7500, the loss=26281.840944944477\n",
      "Current iteration=7600, the loss=26276.559493394707\n",
      "Current iteration=7700, the loss=26271.374011957276\n",
      "Current iteration=7800, the loss=26266.280332350503\n",
      "Current iteration=7900, the loss=26261.274509437855\n",
      "Current iteration=8000, the loss=26256.3528071913\n",
      "Current iteration=8100, the loss=26251.51168567563\n",
      "Current iteration=8200, the loss=26246.74778896915\n",
      "Current iteration=8300, the loss=26242.057933944383\n",
      "Current iteration=8400, the loss=26237.439099838677\n",
      "Current iteration=8500, the loss=26232.888418551873\n",
      "Current iteration=8600, the loss=26228.40316561331\n",
      "Current iteration=8700, the loss=26223.980751765615\n",
      "Current iteration=8800, the loss=26219.618715117416\n",
      "Current iteration=8900, the loss=26215.314713821477\n",
      "Current iteration=9000, the loss=26211.066519238037\n",
      "Current iteration=9100, the loss=26206.87200954704\n",
      "Current iteration=9200, the loss=26202.729163775704\n",
      "Current iteration=9300, the loss=26198.636056210737\n",
      "Current iteration=9400, the loss=26194.59085116718\n",
      "Current iteration=9500, the loss=26190.591798087895\n",
      "Current iteration=9600, the loss=26186.63722695016\n",
      "Current iteration=9700, the loss=26182.725543957356\n",
      "Current iteration=9800, the loss=26178.855227495762\n",
      "Current iteration=9900, the loss=26175.024824337932\n",
      "Current iteration=10000, the loss=26171.232946075637\n",
      "Current iteration=10100, the loss=26167.47826576645\n",
      "Current iteration=10200, the loss=26163.759514779762\n",
      "Current iteration=10300, the loss=26160.075479828476\n",
      "Current iteration=10400, the loss=26156.42500017411\n",
      "Current iteration=10500, the loss=26152.806964993943\n",
      "Current iteration=10600, the loss=26149.220310899313\n",
      "Current iteration=10700, the loss=26145.664019595315\n",
      "Current iteration=10800, the loss=26142.137115673024\n",
      "Current iteration=10900, the loss=26138.63866452517\n",
      "Current iteration=11000, the loss=26135.1677703781\n",
      "Current iteration=11100, the loss=26131.72357443216\n",
      "Current iteration=11200, the loss=26128.30525310399\n",
      "Current iteration=11300, the loss=26124.91201636436\n",
      "Current iteration=11400, the loss=26121.543106165635\n",
      "Current iteration=11500, the loss=26118.19779495331\n",
      "Current iteration=11600, the loss=26114.87538425684\n",
      "Current iteration=11700, the loss=26111.5752033545\n",
      "Current iteration=11800, the loss=26108.29660800837\n",
      "Current iteration=11900, the loss=26105.03897926486\n",
      "Current iteration=12000, the loss=26101.801722317214\n",
      "Current iteration=12100, the loss=26098.58426542626\n",
      "Current iteration=12200, the loss=26095.386058896\n",
      "Current iteration=12300, the loss=26092.20657410099\n",
      "Current iteration=12400, the loss=26089.045302562416\n",
      "Current iteration=12500, the loss=26085.901755070263\n",
      "Current iteration=12600, the loss=26082.775460848825\n",
      "Current iteration=12700, the loss=26079.665966763267\n",
      "Current iteration=12800, the loss=26076.57283656486\n",
      "Current iteration=12900, the loss=26073.495650172896\n",
      "Current iteration=13000, the loss=26070.434002990987\n",
      "Current iteration=13100, the loss=26067.387505256298\n",
      "Current iteration=13200, the loss=26064.35578141958\n",
      "Current iteration=13300, the loss=26061.338469554543\n",
      "Current iteration=13400, the loss=26058.33522079482\n",
      "Current iteration=13500, the loss=26055.34569879745\n",
      "Current iteration=13600, the loss=26052.36957923095\n",
      "Current iteration=13700, the loss=26049.406549287127\n",
      "Current iteration=13800, the loss=26046.4563072152\n",
      "Current iteration=13900, the loss=26043.518561877172\n",
      "Current iteration=14000, the loss=26040.593032323195\n",
      "Current iteration=14100, the loss=26037.67944738619\n",
      "Current iteration=14200, the loss=26034.77754529447\n",
      "Current iteration=14300, the loss=26031.887073301634\n",
      "Current iteration=14400, the loss=26029.00778733284\n",
      "Current iteration=14500, the loss=26026.13945164658\n",
      "Current iteration=14600, the loss=26023.281838511233\n",
      "Current iteration=14700, the loss=26020.434727895758\n",
      "Current iteration=14800, the loss=26017.597907173666\n",
      "Current iteration=14900, the loss=26014.771170839766\n",
      "Current iteration=15000, the loss=26011.95432023896\n",
      "Current iteration=15100, the loss=26009.14716330675\n",
      "Current iteration=15200, the loss=26006.349514320464\n",
      "Current iteration=15300, the loss=26003.561193661182\n",
      "Current iteration=15400, the loss=26000.782027585527\n",
      "Current iteration=15500, the loss=25998.011848006943\n",
      "Current iteration=15600, the loss=25995.250492286144\n",
      "Current iteration=15700, the loss=25992.49780303011\n",
      "Current iteration=15800, the loss=25989.753627899365\n",
      "Current iteration=15900, the loss=25987.017819423185\n",
      "Current iteration=16000, the loss=25984.290234822205\n",
      "Current iteration=16100, the loss=25981.570735838337\n",
      "Current iteration=16200, the loss=25978.859188571463\n",
      "Current iteration=16300, the loss=25976.155463322615\n",
      "Current iteration=16400, the loss=25973.45943444362\n",
      "Current iteration=16500, the loss=25970.770980192483\n",
      "Current iteration=16600, the loss=25968.089982594676\n",
      "Current iteration=16700, the loss=25965.416327309715\n",
      "Current iteration=16800, the loss=25962.749903503216\n",
      "Current iteration=16900, the loss=25960.090603723642\n",
      "Current iteration=17000, the loss=25957.438323784136\n",
      "Current iteration=17100, the loss=25954.792962648695\n",
      "Current iteration=17200, the loss=25952.154422322914\n",
      "Current iteration=17300, the loss=25949.52260774882\n",
      "Current iteration=17400, the loss=25946.89742670374\n",
      "Current iteration=17500, the loss=25944.27878970307\n",
      "Current iteration=17600, the loss=25941.666609906693\n",
      "Current iteration=17700, the loss=25939.060803028973\n",
      "Current iteration=17800, the loss=25936.46128725209\n",
      "Current iteration=17900, the loss=25933.867983142685\n",
      "Current iteration=18000, the loss=25931.28081357156\n",
      "Current iteration=18100, the loss=25928.699703636452\n",
      "Current iteration=18200, the loss=25926.124580587566\n",
      "Current iteration=18300, the loss=25923.55537375599\n",
      "Current iteration=18400, the loss=25920.992014484636\n",
      "Current iteration=18500, the loss=25918.434436061783\n",
      "Current iteration=18600, the loss=25915.882573657043\n",
      "Current iteration=18700, the loss=25913.33636425963\n",
      "Current iteration=18800, the loss=25910.795746618947\n",
      "Current iteration=18900, the loss=25908.260661187254\n",
      "Current iteration=19000, the loss=25905.73105006446\n",
      "Current iteration=19100, the loss=25903.206856944853\n",
      "Current iteration=19200, the loss=25900.6880270658\n",
      "Current iteration=19300, the loss=25898.17450715826\n",
      "Current iteration=19400, the loss=25895.666245399047\n",
      "Current iteration=19500, the loss=25893.163191364823\n",
      "Current iteration=19600, the loss=25890.66529598775\n",
      "Current iteration=19700, the loss=25888.172511512566\n",
      "Current iteration=19800, the loss=25885.684791455402\n",
      "Current iteration=19900, the loss=25883.202090563867\n",
      "Current iteration=20000, the loss=25880.724364778674\n",
      "Current iteration=20100, the loss=25878.251571196415\n",
      "Current iteration=20200, the loss=25875.78366803388\n",
      "Current iteration=20300, the loss=25873.320614593442\n",
      "Current iteration=20400, the loss=25870.862371229716\n",
      "Current iteration=20500, the loss=25868.408899317394\n",
      "Current iteration=20600, the loss=25865.96016122008\n",
      "Current iteration=20700, the loss=25863.51612026039\n",
      "Current iteration=20800, the loss=25861.076740690914\n",
      "Current iteration=20900, the loss=25858.641987666255\n",
      "Current iteration=21000, the loss=25856.211827216004\n",
      "Current iteration=21100, the loss=25853.78622621867\n",
      "Current iteration=21200, the loss=25851.365152376497\n",
      "Current iteration=21300, the loss=25848.948574191047\n",
      "Current iteration=21400, the loss=25846.53646093978\n",
      "Current iteration=21500, the loss=25844.128782653283\n",
      "Current iteration=21600, the loss=25841.725510093405\n",
      "Current iteration=21700, the loss=25839.326614731952\n",
      "Current iteration=21800, the loss=25836.932068730297\n",
      "Current iteration=21900, the loss=25834.54184491956\n",
      "Current iteration=22000, the loss=25832.15591678148\n",
      "Current iteration=22100, the loss=25829.774258430007\n",
      "Current iteration=22200, the loss=25827.396844593335\n",
      "Current iteration=22300, the loss=25825.0236505967\n",
      "Current iteration=22400, the loss=25822.654652345787\n",
      "Current iteration=22500, the loss=25820.289826310473\n",
      "Current iteration=22600, the loss=25817.929149509346\n",
      "Current iteration=22700, the loss=25815.57259949463\n",
      "Current iteration=22800, the loss=25813.220154337625\n",
      "Current iteration=22900, the loss=25810.871792614642\n",
      "Current iteration=23000, the loss=25808.527493393427\n",
      "Current iteration=23100, the loss=25806.187236219994\n",
      "Current iteration=23200, the loss=25803.85100110595\n",
      "Current iteration=23300, the loss=25801.518768516187\n",
      "Current iteration=23400, the loss=25799.190519357042\n",
      "Current iteration=23500, the loss=25796.866234964808\n",
      "Current iteration=23600, the loss=25794.54589709463\n",
      "Current iteration=23700, the loss=25792.22948790981\n",
      "Current iteration=23800, the loss=25789.916989971454\n",
      "Current iteration=23900, the loss=25787.608386228356\n",
      "Current iteration=24000, the loss=25785.30366000746\n",
      "Current iteration=24100, the loss=25783.00279500441\n",
      "Current iteration=24200, the loss=25780.70577527451\n",
      "Current iteration=24300, the loss=25778.41258522397\n",
      "Current iteration=24400, the loss=25776.1232096015\n",
      "Current iteration=24500, the loss=25773.837633490057\n",
      "Current iteration=24600, the loss=25771.555842299058\n",
      "Current iteration=24700, the loss=25769.277821756616\n",
      "Current iteration=24800, the loss=25767.003557902284\n",
      "Current iteration=24900, the loss=25764.73303707987\n",
      "The loss=25762.48889542326\n",
      "Training error: 25887.128922994914\n",
      "Test error: 8706.323389396612\n",
      "Classification accuracy: 0.7969024037965542\n"
     ]
    }
   ],
   "source": [
    "from toolbox import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters_1 = 25000\n",
    "gamma_1 = 0.9e-7\n",
    "lambda_1 = 0\n",
    "degree_1 = 3\n",
    "batch_size1 = 100\n",
    "k = 4\n",
    "current_y = y1\n",
    "current_X = X1\n",
    "k_indices = build_k_indices(current_y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (current_y[test_indices], current_X[test_indices])\n",
    "    test_x = build_poly_cos(test_x, degree_1, np.array(range(1,23)))\n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (current_y[training_indices], current_X[training_indices])\n",
    "    training_x = build_poly_cos(training_x, degree_1, np.array(range(1,23)))\n",
    "\n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_1, gamma_1, max_iters_1)\n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    test_y[test_y == 0] = -1\n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "    test_y[test_y == -1] = 0\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))\n",
    "overall_acc.append(np.mean(classification_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=36408.48467602852\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-cca91ab8dc5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mtraining_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_poly_cos_sin_log_poly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mw_star\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iters_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# compute classification accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bojan\\Desktop\\ml-group78.git\\scripts\\toolbox.py\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, gamma, max_iters)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m# start the logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mgradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlambda_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# get loss and updated w\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\bojan\\Desktop\\ml-group78.git\\scripts\\toolbox.py\u001b[0m in \u001b[0;36mcalculate_gradient_log_likelihood\u001b[1;34m(y, tx, w)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcalculate_gradient_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;34m\"\"\"compute the gradient of negative log likelihood.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[1;31m# stochastic gradient descent helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from toolbox import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters_2 = 15000\n",
    "gamma_2 = 1e-7\n",
    "lambda_2 = 0.0\n",
    "degree_2 = 3\n",
    "batch_size2 = 100\n",
    "k = 4\n",
    "current_y = y2\n",
    "current_X = X2\n",
    "k_indices = build_k_indices(current_y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (current_y[test_indices], current_X[test_indices])\n",
    "    test_x = build_poly_cos_sin_log_poly(test_x, degree_2, np.array(range(1,50)))\n",
    "    \n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (current_y[training_indices], current_X[training_indices])\n",
    "    training_x = build_poly_cos_sin_log_poly(training_x, degree_2, np.array(range(1,50)))\n",
    "    \n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_2, gamma_2, max_iters_2)\n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    test_y[test_y == 0] = -1\n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "    test_y[test_y == -1] = 0\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))\n",
    "overall_acc.append(np.mean(classification_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from toolbox import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters_3 = 12000\n",
    "gamma_3 = 5e-7\n",
    "lambda_3 = 0\n",
    "degree_3 = 3\n",
    "batch_size3 = 100\n",
    "k = 2\n",
    "current_y = y3\n",
    "current_X = X3\n",
    "k_indices = build_k_indices(current_y, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (current_y[test_indices], current_X[test_indices])\n",
    "    test_x = build_poly(test_x, degree_3)\n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (current_y[training_indices], current_X[training_indices])\n",
    "    training_x = build_poly(training_x, degree_3)\n",
    "\n",
    "    w_star = reg_logistic_regression(training_y, training_x, lambda_3, gamma_3, max_iters_3)\n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    test_y[test_y == 0] = -1\n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "    test_y[test_y == -1] = 0\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))\n",
    "overall_acc.append(np.mean(classification_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall accuracy: 0.8007403114973887\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The overall accuracy: {acc}\\n\".format(acc=np.mean(overall_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=54340.191702038836\n",
      "Current iteration=100, the loss=37952.804684479896\n",
      "Current iteration=200, the loss=37321.30942410637\n",
      "Current iteration=300, the loss=36921.56647140509\n",
      "Current iteration=400, the loss=36628.11460903473\n",
      "Current iteration=500, the loss=36400.89071359332\n",
      "Current iteration=600, the loss=36218.662962899674\n",
      "Current iteration=700, the loss=36068.84766433046\n",
      "Current iteration=800, the loss=35943.58157519234\n",
      "Current iteration=900, the loss=35837.49564183938\n",
      "Current iteration=1000, the loss=35746.68895511683\n",
      "Current iteration=1100, the loss=35668.23176921406\n",
      "Current iteration=1200, the loss=35599.87887419586\n",
      "Current iteration=1300, the loss=35539.88352989149\n",
      "Current iteration=1400, the loss=35486.86938306488\n",
      "Current iteration=1500, the loss=35439.73940980333\n",
      "Current iteration=1600, the loss=35397.60986723072\n",
      "Current iteration=1700, the loss=35359.76171272335\n",
      "Current iteration=1800, the loss=35325.604468390564\n",
      "Current iteration=1900, the loss=35294.64906342549\n",
      "Current iteration=2000, the loss=35266.48721323699\n",
      "Current iteration=2100, the loss=35240.77559892541\n",
      "Current iteration=2200, the loss=35217.22360334947\n",
      "Current iteration=2300, the loss=35195.583707116326\n",
      "Current iteration=2400, the loss=35175.643893483495\n",
      "Current iteration=2500, the loss=35157.22158601615\n",
      "Current iteration=2600, the loss=35140.15876819572\n",
      "Current iteration=2700, the loss=35124.31802466414\n",
      "Current iteration=2800, the loss=35109.579309353605\n",
      "Current iteration=2900, the loss=35095.837293194345\n",
      "Current iteration=3000, the loss=35082.99917825307\n",
      "Current iteration=3100, the loss=35070.9828897117\n",
      "Current iteration=3200, the loss=35059.71557496535\n",
      "Current iteration=3300, the loss=35049.132352558576\n",
      "Current iteration=3400, the loss=35039.175264283804\n",
      "Current iteration=3500, the loss=35029.792392495314\n",
      "Current iteration=3600, the loss=35020.937112028405\n",
      "Current iteration=3700, the loss=35012.56745225204\n",
      "Current iteration=3800, the loss=35004.645549819135\n",
      "Current iteration=3900, the loss=34997.13717670409\n",
      "Current iteration=4000, the loss=34990.01133126249\n",
      "Current iteration=4100, the loss=34983.23988246626\n",
      "Current iteration=4200, the loss=34976.79725931844\n",
      "Current iteration=4300, the loss=34970.66017887372\n",
      "Current iteration=4400, the loss=34964.80740739955\n",
      "Current iteration=4500, the loss=34959.21955009197\n",
      "Current iteration=4600, the loss=34953.87886547226\n",
      "Current iteration=4700, the loss=34948.76910117565\n",
      "Current iteration=4800, the loss=34943.875348330905\n",
      "Current iteration=4900, the loss=34939.18391213854\n",
      "Current iteration=5000, the loss=34934.68219660093\n",
      "Current iteration=5100, the loss=34930.35860164848\n",
      "Current iteration=5200, the loss=34926.202431153055\n",
      "Current iteration=5300, the loss=34922.20381052825\n",
      "Current iteration=5400, the loss=34918.353612792635\n",
      "Current iteration=5500, the loss=34914.64339212201\n",
      "Current iteration=5600, the loss=34911.06532404374\n",
      "Current iteration=5700, the loss=34907.61215153457\n",
      "Current iteration=5800, the loss=34904.27713637585\n",
      "Current iteration=5900, the loss=34901.054015199086\n",
      "Current iteration=6000, the loss=34897.93695972308\n",
      "Current iteration=6100, the loss=34894.92054074298\n",
      "Current iteration=6200, the loss=34891.9996954816\n",
      "Current iteration=6300, the loss=34889.1696979589\n",
      "Current iteration=6400, the loss=34886.42613207311\n",
      "Current iteration=6500, the loss=34883.76486712159\n",
      "Current iteration=6600, the loss=34881.18203551839\n",
      "Current iteration=6700, the loss=34878.674012492724\n",
      "Current iteration=6800, the loss=34876.23739757415\n",
      "Current iteration=6900, the loss=34873.86899769223\n",
      "Current iteration=7000, the loss=34871.5658117352\n",
      "Current iteration=7100, the loss=34869.325016428935\n",
      "Current iteration=7200, the loss=34867.14395341148\n",
      "Current iteration=7300, the loss=34865.020117391134\n",
      "Current iteration=7400, the loss=34862.9511452874\n",
      "Current iteration=7500, the loss=34860.93480626424\n",
      "Current iteration=7600, the loss=34858.96899257397\n",
      "Current iteration=7700, the loss=34857.05171113831\n",
      "Current iteration=7800, the loss=34855.18107580015\n",
      "Current iteration=7900, the loss=34853.355300186406\n",
      "Current iteration=8000, the loss=34851.572691127556\n",
      "Current iteration=8100, the loss=34849.831642585414\n",
      "Current iteration=8200, the loss=34848.13063004447\n",
      "Current iteration=8300, the loss=34846.46820532718\n",
      "Current iteration=8400, the loss=34844.84299179657\n",
      "Current iteration=8500, the loss=34843.25367991384\n",
      "Current iteration=8600, the loss=34841.69902312054\n",
      "Current iteration=8700, the loss=34840.17783401881\n",
      "Current iteration=8800, the loss=34838.68898082477\n",
      "Current iteration=8900, the loss=34837.23138407285\n",
      "Current iteration=9000, the loss=34835.804013550776\n",
      "Current iteration=9100, the loss=34834.4058854468\n",
      "Current iteration=9200, the loss=34833.03605969213\n",
      "Current iteration=9300, the loss=34831.69363748327\n",
      "Current iteration=9400, the loss=34830.377758970695\n",
      "Current iteration=9500, the loss=34829.08760110024\n",
      "Current iteration=9600, the loss=34827.822375596545\n",
      "Current iteration=9700, the loss=34826.58132707702\n",
      "Current iteration=9800, the loss=34825.36373128734\n",
      "Current iteration=9900, the loss=34824.16889344885\n",
      "Current iteration=10000, the loss=34822.996146710284\n",
      "Current iteration=10100, the loss=34821.84485069616\n",
      "Current iteration=10200, the loss=34820.714390144596\n",
      "Current iteration=10300, the loss=34819.60417362897\n",
      "Current iteration=10400, the loss=34818.51363235689\n",
      "Current iteration=10500, the loss=34817.44221904168\n",
      "Current iteration=10600, the loss=34816.38940684133\n",
      "Current iteration=10700, the loss=34815.354688360145\n",
      "Current iteration=10800, the loss=34814.337574709156\n",
      "Current iteration=10900, the loss=34813.33759462161\n",
      "Current iteration=11000, the loss=34812.354293619384\n",
      "Current iteration=11100, the loss=34811.387233227724\n",
      "Current iteration=11200, the loss=34810.4359902346\n",
      "Current iteration=11300, the loss=34809.500155992275\n",
      "Current iteration=11400, the loss=34808.57933575831\n",
      "Current iteration=11500, the loss=34807.67314807334\n",
      "Current iteration=11600, the loss=34806.78122417365\n",
      "Current iteration=11700, the loss=34805.90320743611\n",
      "Current iteration=11800, the loss=34805.03875285371\n",
      "Current iteration=11900, the loss=34804.18752653959\n",
      "Current iteration=12000, the loss=34803.34920525791\n",
      "Current iteration=12100, the loss=34802.52347598012\n",
      "Current iteration=12200, the loss=34801.71003546457\n",
      "Current iteration=12300, the loss=34800.908589858656\n",
      "Current iteration=12400, the loss=34800.118854321685\n",
      "Current iteration=12500, the loss=34799.34055266723\n",
      "Current iteration=12600, the loss=34798.57341702419\n",
      "Current iteration=12700, the loss=34797.817187514855\n",
      "Current iteration=12800, the loss=34797.07161194937\n",
      "Current iteration=12900, the loss=34796.33644553546\n",
      "Current iteration=13000, the loss=34795.61145060241\n",
      "Current iteration=13100, the loss=34794.89639633866\n",
      "Current iteration=13200, the loss=34794.191058541925\n",
      "Current iteration=13300, the loss=34793.495219381395\n",
      "Current iteration=13400, the loss=34792.80866717101\n",
      "Current iteration=13500, the loss=34792.131196153394\n",
      "Current iteration=13600, the loss=34791.46260629357\n",
      "Current iteration=13700, the loss=34790.80270308218\n",
      "Current iteration=13800, the loss=34790.15129734746\n",
      "Current iteration=13900, the loss=34789.50820507541\n",
      "Current iteration=14000, the loss=34788.873247237934\n",
      "Current iteration=14100, the loss=34788.24624962831\n",
      "Current iteration=14200, the loss=34787.62704270355\n",
      "Current iteration=14300, the loss=34787.01546143344\n",
      "Current iteration=14400, the loss=34786.41134515577\n",
      "Current iteration=14500, the loss=34785.81453743743\n",
      "Current iteration=14600, the loss=34785.22488594112\n",
      "Current iteration=14700, the loss=34784.64224229715\n",
      "Current iteration=14800, the loss=34784.06646198062\n",
      "Current iteration=14900, the loss=34783.497404192865\n",
      "The loss=34782.940524316626\n",
      "Current iteration=0, the loss=52363.87794773491\n",
      "Current iteration=100, the loss=42716.54522787409\n",
      "Current iteration=200, the loss=39822.38975744616\n",
      "Current iteration=300, the loss=39073.891493700125\n",
      "Current iteration=400, the loss=37484.9744852578\n",
      "Current iteration=500, the loss=37046.07269012437\n",
      "Current iteration=600, the loss=36435.58523583614\n",
      "Current iteration=700, the loss=36110.58748219533\n",
      "Current iteration=800, the loss=36637.86203129092\n",
      "Current iteration=900, the loss=36358.05224332493\n",
      "Current iteration=1000, the loss=36241.274194360994\n",
      "Current iteration=1100, the loss=35415.55615614254\n",
      "Current iteration=1200, the loss=35336.682804215525\n",
      "Current iteration=1300, the loss=35848.169061520995\n",
      "Current iteration=1400, the loss=35643.25646700688\n",
      "Current iteration=1500, the loss=35518.475377775496\n",
      "Current iteration=1600, the loss=34978.18038003327\n",
      "Current iteration=1700, the loss=35461.20817346353\n",
      "Current iteration=1800, the loss=34829.88485805433\n",
      "Current iteration=1900, the loss=35187.16400350137\n",
      "Current iteration=2000, the loss=35251.54601442138\n",
      "Current iteration=2100, the loss=34606.22654882676\n",
      "Current iteration=2200, the loss=34924.00495390878\n",
      "Current iteration=2300, the loss=34518.46957534769\n",
      "Current iteration=2400, the loss=34922.96951168958\n",
      "Current iteration=2500, the loss=34391.15840059009\n",
      "Current iteration=2600, the loss=34336.366305418574\n",
      "Current iteration=2700, the loss=34409.557296454725\n",
      "Current iteration=2800, the loss=34424.13282691429\n",
      "Current iteration=2900, the loss=34558.802839753094\n",
      "Current iteration=3000, the loss=34285.45281539118\n",
      "Current iteration=3100, the loss=34228.072557313215\n",
      "Current iteration=3200, the loss=34252.08032512984\n",
      "Current iteration=3300, the loss=34178.27913823877\n",
      "Current iteration=3400, the loss=34188.54007326263\n",
      "Current iteration=3500, the loss=34135.47151367319\n",
      "Current iteration=3600, the loss=34131.61865464876\n",
      "Current iteration=3700, the loss=34097.44485195832\n",
      "Current iteration=3800, the loss=34081.05004512398\n",
      "Current iteration=3900, the loss=34059.2959017534\n",
      "Current iteration=4000, the loss=34039.75164176668\n",
      "Current iteration=4100, the loss=34020.684897139436\n",
      "Current iteration=4200, the loss=34002.267260677516\n",
      "Current iteration=4300, the loss=33984.45831862629\n",
      "Current iteration=4400, the loss=33967.23589651757\n",
      "Current iteration=4500, the loss=33950.581595268\n",
      "Current iteration=4600, the loss=33934.48066214686\n",
      "Current iteration=4700, the loss=33918.92162549012\n",
      "Current iteration=4800, the loss=33903.89604270508\n",
      "Current iteration=4900, the loss=33889.39835293777\n",
      "Current iteration=5000, the loss=33875.42584717776\n",
      "Current iteration=5100, the loss=33861.97877723014\n",
      "Current iteration=5200, the loss=33849.06063018266\n",
      "Current iteration=5300, the loss=33836.67859798485\n",
      "Current iteration=5400, the loss=33824.844272248716\n",
      "Current iteration=5500, the loss=33813.57458994901\n",
      "Current iteration=5600, the loss=33802.893039877345\n",
      "Current iteration=5700, the loss=33792.83109930137\n",
      "Current iteration=5800, the loss=33783.429784893626\n",
      "Current iteration=5900, the loss=33774.74105795325\n",
      "Current iteration=6000, the loss=33766.82868045207\n",
      "Current iteration=6100, the loss=33759.768303194265\n",
      "Current iteration=6200, the loss=33753.64802838853\n",
      "Current iteration=6300, the loss=33748.57504853654\n",
      "Current iteration=6400, the loss=33744.70213175279\n",
      "Current iteration=6500, the loss=33742.29918109322\n",
      "Current iteration=6600, the loss=33741.92307386045\n",
      "Current iteration=6700, the loss=33744.88545078478\n",
      "Current iteration=6800, the loss=33755.23453707369\n",
      "Current iteration=6900, the loss=33796.87383145554\n",
      "Current iteration=7000, the loss=33810.533398794956\n",
      "Current iteration=7100, the loss=33798.16818698295\n",
      "Current iteration=7200, the loss=33786.70645458169\n",
      "Current iteration=7300, the loss=33775.78596382101\n",
      "Current iteration=7400, the loss=33765.23929468615\n",
      "Current iteration=7500, the loss=33754.980335444125\n",
      "Current iteration=7600, the loss=33744.96185818358\n",
      "Current iteration=7700, the loss=33735.156371972844\n",
      "Current iteration=7800, the loss=33725.54673959224\n",
      "Current iteration=7900, the loss=33716.11831865331\n",
      "Current iteration=8000, the loss=33707.16459952492\n",
      "Current iteration=8100, the loss=33675.023680509286\n",
      "Current iteration=8200, the loss=33685.202075826586\n",
      "Current iteration=8300, the loss=33699.418821914405\n",
      "Current iteration=8400, the loss=33702.13433006435\n",
      "Current iteration=8500, the loss=33696.63785378036\n",
      "Current iteration=8600, the loss=33687.92448923251\n",
      "Current iteration=8700, the loss=33671.450398680376\n",
      "Current iteration=8800, the loss=33642.58360425342\n",
      "Current iteration=8900, the loss=33610.224917772124\n",
      "Current iteration=9000, the loss=33592.31479959734\n",
      "Current iteration=9100, the loss=33594.053929260845\n",
      "Current iteration=9200, the loss=33641.01853329032\n",
      "Current iteration=9300, the loss=33621.35349091682\n",
      "Current iteration=9400, the loss=33560.64540583148\n",
      "Current iteration=9500, the loss=33564.80772676163\n",
      "Current iteration=9600, the loss=33623.57537901274\n",
      "Current iteration=9700, the loss=33552.02186659288\n",
      "Current iteration=9800, the loss=33534.549263353576\n",
      "Current iteration=9900, the loss=33604.22981758973\n",
      "Current iteration=10000, the loss=33526.69327689266\n",
      "Current iteration=10100, the loss=33521.418541041276\n",
      "Current iteration=10200, the loss=33586.113998275774\n",
      "Current iteration=10300, the loss=33493.35930996624\n",
      "Current iteration=10400, the loss=33524.72369840249\n",
      "Current iteration=10500, the loss=33555.115265054046\n",
      "Current iteration=10600, the loss=33470.482017811344\n",
      "Current iteration=10700, the loss=33526.83509079467\n",
      "Current iteration=10800, the loss=33525.752725617174\n",
      "Current iteration=10900, the loss=33450.798502800666\n",
      "Current iteration=11000, the loss=33504.271000307606\n",
      "Current iteration=11100, the loss=33525.5431704712\n",
      "Current iteration=11200, the loss=33436.870911342136\n",
      "Current iteration=11300, the loss=33443.215471149655\n",
      "Current iteration=11400, the loss=33507.08773877847\n",
      "Current iteration=11500, the loss=33503.32463371764\n",
      "Current iteration=11600, the loss=33440.74834763795\n",
      "Current iteration=11700, the loss=33406.56374058154\n",
      "Current iteration=11800, the loss=33401.329597924494\n",
      "Current iteration=11900, the loss=33397.94304003673\n",
      "Current iteration=12000, the loss=33392.99861524349\n",
      "Current iteration=12100, the loss=33398.42586307942\n",
      "Current iteration=12200, the loss=33461.7529998576\n",
      "Current iteration=12300, the loss=33422.39320292535\n",
      "Current iteration=12400, the loss=33397.28314459305\n",
      "Current iteration=12500, the loss=33390.983422485624\n",
      "Current iteration=12600, the loss=33437.05439161215\n",
      "Current iteration=12700, the loss=33436.13232550261\n",
      "Current iteration=12800, the loss=33388.84331798082\n",
      "Current iteration=12900, the loss=33372.90176079618\n",
      "Current iteration=13000, the loss=33369.81117678214\n",
      "Current iteration=13100, the loss=33388.63167576246\n",
      "Current iteration=13200, the loss=33349.52306218732\n",
      "Current iteration=13300, the loss=33402.432223053576\n",
      "Current iteration=13400, the loss=33336.2633739205\n",
      "Current iteration=13500, the loss=33400.52107857233\n",
      "Current iteration=13600, the loss=33329.064885001506\n",
      "Current iteration=13700, the loss=33395.417173302296\n",
      "Current iteration=13800, the loss=33331.49112732505\n",
      "Current iteration=13900, the loss=33329.3823301173\n",
      "Current iteration=14000, the loss=33344.73653959195\n",
      "Current iteration=14100, the loss=33338.525344614405\n",
      "Current iteration=14200, the loss=33355.029969162904\n",
      "Current iteration=14300, the loss=33338.111868114545\n",
      "Current iteration=14400, the loss=33339.14517603271\n",
      "Current iteration=14500, the loss=33347.60825532325\n",
      "Current iteration=14600, the loss=33307.50348876304\n",
      "Current iteration=14700, the loss=33326.52941569894\n",
      "Current iteration=14800, the loss=33330.081916784366\n",
      "Current iteration=14900, the loss=33322.44130718577\n",
      "Current iteration=15000, the loss=33302.99775073833\n",
      "Current iteration=15100, the loss=33310.21933104188\n",
      "Current iteration=15200, the loss=33303.33591248655\n",
      "Current iteration=15300, the loss=33299.17608327775\n",
      "Current iteration=15400, the loss=33295.95115592461\n",
      "Current iteration=15500, the loss=33292.79780598548\n",
      "Current iteration=15600, the loss=33289.77614905184\n",
      "Current iteration=15700, the loss=33286.99900916558\n",
      "Current iteration=15800, the loss=33286.15630207132\n",
      "Current iteration=15900, the loss=33273.24362248599\n",
      "Current iteration=16000, the loss=33281.98404079911\n",
      "Current iteration=16100, the loss=33278.130505503555\n",
      "Current iteration=16200, the loss=33274.790106371074\n",
      "Current iteration=16300, the loss=33271.66968647594\n",
      "Current iteration=16400, the loss=33268.564831596115\n",
      "Current iteration=16500, the loss=33265.47097923775\n",
      "Current iteration=16600, the loss=33262.38890996227\n",
      "Current iteration=16700, the loss=33259.31954777834\n",
      "Current iteration=16800, the loss=33256.2636980649\n",
      "Current iteration=16900, the loss=33253.22204152001\n",
      "Current iteration=17000, the loss=33250.19514582095\n",
      "Current iteration=17100, the loss=33247.1834803133\n",
      "Current iteration=17200, the loss=33244.1874303629\n",
      "Current iteration=17300, the loss=33241.2073102038\n",
      "Current iteration=17400, the loss=33238.24337397382\n",
      "Current iteration=17500, the loss=33235.29582500512\n",
      "Current iteration=17600, the loss=33232.36482357133\n",
      "Current iteration=17700, the loss=33229.450493326556\n",
      "Current iteration=17800, the loss=33226.552926661025\n",
      "Current iteration=17900, the loss=33223.6721891569\n",
      "Current iteration=18000, the loss=33220.808320775526\n",
      "Current iteration=18100, the loss=33217.961137069746\n",
      "Current iteration=18200, the loss=33215.145102616654\n",
      "Current iteration=18300, the loss=33220.24926085524\n",
      "Current iteration=18400, the loss=33190.38742857341\n",
      "Current iteration=18500, the loss=33183.602748521706\n",
      "Current iteration=18600, the loss=33194.842256858785\n",
      "Current iteration=18700, the loss=33222.16323591748\n",
      "Current iteration=18800, the loss=33223.96289446515\n",
      "Current iteration=18900, the loss=33222.79138849236\n",
      "Current iteration=19000, the loss=33221.01012937325\n",
      "Current iteration=19100, the loss=33219.457312355706\n",
      "Current iteration=19200, the loss=33213.79224227252\n",
      "Current iteration=19300, the loss=33168.33619287207\n",
      "Current iteration=19400, the loss=33138.248594239114\n",
      "Current iteration=19500, the loss=33172.52608325134\n",
      "Current iteration=19600, the loss=33205.8918933236\n",
      "Current iteration=19700, the loss=33146.50251608806\n",
      "Current iteration=19800, the loss=33165.95289932245\n",
      "Current iteration=19900, the loss=33201.638982920136\n",
      "Current iteration=20000, the loss=33145.50385513305\n",
      "Current iteration=20100, the loss=33195.39975303676\n",
      "Current iteration=20200, the loss=33169.86429166308\n",
      "Current iteration=20300, the loss=33117.10469941054\n",
      "Current iteration=20400, the loss=33192.61970940172\n",
      "Current iteration=20500, the loss=33158.51574126975\n",
      "Current iteration=20600, the loss=33098.65985375467\n",
      "Current iteration=20700, the loss=33127.146153864756\n",
      "Current iteration=20800, the loss=33181.121227042386\n",
      "Current iteration=20900, the loss=33184.11696655049\n",
      "Current iteration=21000, the loss=33177.71670368101\n",
      "Current iteration=21100, the loss=33174.63689501792\n",
      "Current iteration=21200, the loss=33177.419916556035\n",
      "Current iteration=21300, the loss=33169.52834060678\n",
      "Current iteration=21400, the loss=33108.387039782785\n",
      "Current iteration=21500, the loss=33075.03473803391\n",
      "Current iteration=21600, the loss=33151.96222554728\n",
      "Current iteration=21700, the loss=33166.519196404835\n",
      "Current iteration=21800, the loss=33060.60713382613\n",
      "Current iteration=21900, the loss=33147.7373903091\n",
      "Current iteration=22000, the loss=33132.85368414269\n",
      "Current iteration=22100, the loss=33125.03639416715\n",
      "Current iteration=22200, the loss=33134.70786400119\n",
      "Current iteration=22300, the loss=33134.34934355925\n",
      "Current iteration=22400, the loss=33072.896057444545\n",
      "Current iteration=22500, the loss=33145.194945786876\n",
      "Current iteration=22600, the loss=33077.636766509975\n",
      "Current iteration=22700, the loss=33082.63278507302\n",
      "Current iteration=22800, the loss=33136.995890578706\n",
      "Current iteration=22900, the loss=33120.25316423824\n",
      "Current iteration=23000, the loss=33027.53245949391\n",
      "Current iteration=23100, the loss=33067.302651779566\n",
      "Current iteration=23200, the loss=33131.77642316479\n",
      "Current iteration=23300, the loss=33131.457240912074\n",
      "Current iteration=23400, the loss=33126.3603146727\n",
      "Current iteration=23500, the loss=33107.324176609574\n",
      "Current iteration=23600, the loss=33073.58165141946\n",
      "Current iteration=23700, the loss=33059.83207237372\n",
      "Current iteration=23800, the loss=33032.047369271495\n",
      "Current iteration=23900, the loss=33019.96783119013\n",
      "Current iteration=24000, the loss=33021.66702115762\n",
      "Current iteration=24100, the loss=33023.693406127364\n",
      "Current iteration=24200, the loss=33026.455443303435\n",
      "Current iteration=24300, the loss=33032.37844960962\n",
      "Current iteration=24400, the loss=33060.2024417146\n",
      "Current iteration=24500, the loss=33084.527227321414\n",
      "Current iteration=24600, the loss=33102.62507537304\n",
      "Current iteration=24700, the loss=33105.03509592463\n",
      "Current iteration=24800, the loss=33098.86912579459\n",
      "Current iteration=24900, the loss=33075.84546907122\n",
      "The loss=32864.31921049698\n",
      "Current iteration=0, the loss=48332.11479013386\n",
      "Current iteration=100, the loss=39072.353700775275\n",
      "Current iteration=200, the loss=36647.7501778037\n",
      "Current iteration=300, the loss=35205.4577627192\n",
      "Current iteration=400, the loss=34208.43863926002\n",
      "Current iteration=500, the loss=33454.28784955329\n",
      "Current iteration=600, the loss=32877.58376147653\n",
      "Current iteration=700, the loss=32395.095926220867\n",
      "Current iteration=800, the loss=32016.704728310575\n",
      "Current iteration=900, the loss=31692.546995199642\n",
      "Current iteration=1000, the loss=31410.192049548787\n",
      "Current iteration=1100, the loss=31176.34416008418\n",
      "Current iteration=1200, the loss=30979.19824340017\n",
      "Current iteration=1300, the loss=30801.034014907753\n",
      "Current iteration=1400, the loss=30635.693342047685\n",
      "Current iteration=1500, the loss=30508.510541878473\n",
      "Current iteration=1600, the loss=30378.387107812974\n",
      "Current iteration=1700, the loss=30266.38798846882\n",
      "Current iteration=1800, the loss=30157.789544888386\n",
      "Current iteration=1900, the loss=30064.79252995339\n",
      "Current iteration=2000, the loss=29981.94122244187\n",
      "Current iteration=2100, the loss=29901.218089970942\n",
      "Current iteration=2200, the loss=29832.424654692222\n",
      "Current iteration=2300, the loss=29775.742994809523\n",
      "Current iteration=2400, the loss=29701.85704982464\n",
      "Current iteration=2500, the loss=29643.349700622057\n",
      "Current iteration=2600, the loss=29598.736119295856\n",
      "Current iteration=2700, the loss=29548.765666920423\n",
      "Current iteration=2800, the loss=29500.104009989038\n",
      "Current iteration=2900, the loss=29453.638895105483\n",
      "Current iteration=3000, the loss=29415.554755067205\n",
      "Current iteration=3100, the loss=29365.394146794424\n",
      "Current iteration=3200, the loss=29323.76193071167\n",
      "Current iteration=3300, the loss=29287.603340059082\n",
      "Current iteration=3400, the loss=29262.38052663533\n",
      "Current iteration=3500, the loss=29226.716495720117\n",
      "Current iteration=3600, the loss=29195.084018854723\n",
      "Current iteration=3700, the loss=29164.207369259624\n",
      "Current iteration=3800, the loss=29128.900642312903\n",
      "Current iteration=3900, the loss=29105.161731212604\n",
      "Current iteration=4000, the loss=29078.78097770676\n",
      "Current iteration=4100, the loss=29043.473443443043\n",
      "Current iteration=4200, the loss=29026.9526900164\n",
      "Current iteration=4300, the loss=29001.27552434362\n",
      "Current iteration=4400, the loss=28968.59509120416\n",
      "Current iteration=4500, the loss=28953.37932788668\n",
      "Current iteration=4600, the loss=28930.202617342657\n",
      "Current iteration=4700, the loss=28900.047228516327\n",
      "Current iteration=4800, the loss=28887.84150940823\n",
      "Current iteration=4900, the loss=28866.035427531075\n",
      "Current iteration=5000, the loss=28837.639515627525\n",
      "Current iteration=5100, the loss=28825.4099548156\n",
      "Current iteration=5200, the loss=28807.774146115233\n",
      "Current iteration=5300, the loss=28781.557875565995\n",
      "Current iteration=5400, the loss=28763.44814600653\n",
      "Current iteration=5500, the loss=28754.378775426798\n",
      "Current iteration=5600, the loss=28728.68265544492\n",
      "Current iteration=5700, the loss=28708.696494919976\n",
      "Current iteration=5800, the loss=28702.88289211497\n",
      "Current iteration=5900, the loss=28677.507068966028\n",
      "Current iteration=6000, the loss=28658.609307518265\n",
      "Current iteration=6100, the loss=28653.644174322122\n",
      "Current iteration=6200, the loss=28627.086186018692\n",
      "Current iteration=6300, the loss=28616.025237635447\n",
      "Current iteration=6400, the loss=28599.402262159274\n",
      "Current iteration=6500, the loss=28594.12177406111\n",
      "Current iteration=6600, the loss=28571.135277189795\n",
      "Current iteration=6700, the loss=28553.60063396202\n",
      "Current iteration=6800, the loss=28544.326244530137\n",
      "Current iteration=6900, the loss=28526.956123383716\n",
      "Current iteration=7000, the loss=28521.165664262226\n",
      "Current iteration=7100, the loss=28500.451261776456\n",
      "Current iteration=7200, the loss=28500.377902939726\n",
      "Current iteration=7300, the loss=28487.890733745502\n",
      "Current iteration=7400, the loss=28475.77149955606\n",
      "Current iteration=7500, the loss=28463.892686554143\n",
      "Current iteration=7600, the loss=28452.12784864742\n",
      "Current iteration=7700, the loss=28440.529480484664\n",
      "Current iteration=7800, the loss=28429.126412828497\n",
      "Current iteration=7900, the loss=28417.930394424664\n",
      "Current iteration=8000, the loss=28406.94357498733\n",
      "Current iteration=8100, the loss=28396.154269969054\n",
      "Current iteration=8200, the loss=28385.527422117317\n",
      "Current iteration=8300, the loss=28375.013529816315\n",
      "Current iteration=8400, the loss=28364.591712669688\n",
      "Current iteration=8500, the loss=28354.27598284652\n",
      "Current iteration=8600, the loss=28344.08491041766\n",
      "Current iteration=8700, the loss=28334.030050158588\n",
      "Current iteration=8800, the loss=28324.117094415647\n",
      "Current iteration=8900, the loss=28314.336102402078\n",
      "Current iteration=9000, the loss=28295.77900638232\n",
      "Current iteration=9100, the loss=28285.17160464356\n",
      "Current iteration=9200, the loss=28271.815459351696\n",
      "Current iteration=9300, the loss=28263.99207042466\n",
      "Current iteration=9400, the loss=28261.072079279562\n",
      "Current iteration=9500, the loss=28244.59143575922\n",
      "Current iteration=9600, the loss=28239.802292749722\n",
      "Current iteration=9700, the loss=28240.424514507264\n",
      "Current iteration=9800, the loss=28232.854699601405\n",
      "Current iteration=9900, the loss=28215.73486659689\n",
      "Current iteration=10000, the loss=28205.587576757935\n",
      "Current iteration=10100, the loss=28207.285472517266\n",
      "Current iteration=10200, the loss=28190.2731439496\n",
      "Current iteration=10300, the loss=28181.082069521246\n",
      "Current iteration=10400, the loss=28174.767220693735\n",
      "Current iteration=10500, the loss=28167.299129504274\n",
      "Current iteration=10600, the loss=28170.82077963926\n",
      "Current iteration=10700, the loss=28148.69912649376\n",
      "Current iteration=10800, the loss=28143.283110397497\n",
      "Current iteration=10900, the loss=28134.971073694134\n",
      "Current iteration=11000, the loss=28143.730640651185\n",
      "Current iteration=11100, the loss=28120.343661171675\n",
      "Current iteration=11200, the loss=28114.02109835588\n",
      "Current iteration=11300, the loss=28108.74164062168\n",
      "Current iteration=11400, the loss=28100.159131728244\n",
      "Current iteration=11500, the loss=28109.701658123144\n",
      "Current iteration=11600, the loss=28105.31236527839\n",
      "Current iteration=11700, the loss=28081.17601367773\n",
      "Current iteration=11800, the loss=28092.180826997792\n",
      "Current iteration=11900, the loss=28067.136453081475\n",
      "Current iteration=12000, the loss=28065.56143809855\n",
      "Current iteration=12100, the loss=28073.262445180095\n",
      "Current iteration=12200, the loss=28048.612524307035\n",
      "Current iteration=12300, the loss=28047.143326977588\n",
      "Current iteration=12400, the loss=28055.0679038991\n",
      "Current iteration=12500, the loss=28030.768811761227\n",
      "Current iteration=12600, the loss=28029.41003899387\n",
      "Current iteration=12700, the loss=28037.537940195034\n",
      "Current iteration=12800, the loss=28013.569295317546\n",
      "Current iteration=12900, the loss=28012.323555834577\n",
      "Current iteration=13000, the loss=28020.62898406742\n",
      "Current iteration=13100, the loss=27996.98094605151\n",
      "Current iteration=13200, the loss=27995.850006789216\n",
      "Current iteration=13300, the loss=28004.30938694778\n",
      "Current iteration=13400, the loss=27980.973106573118\n",
      "Current iteration=13500, the loss=27979.954145534706\n",
      "Current iteration=13600, the loss=27988.555029864976\n",
      "Current iteration=13700, the loss=27965.516379170585\n",
      "Current iteration=13800, the loss=27964.607366892218\n",
      "Current iteration=13900, the loss=27973.33030728026\n",
      "Current iteration=14000, the loss=27950.582770629902\n",
      "Current iteration=14100, the loss=27949.783047476558\n",
      "Current iteration=14200, the loss=27958.606828928387\n",
      "Current iteration=14300, the loss=27936.1463458325\n",
      "Current iteration=14400, the loss=27935.45523220839\n",
      "Current iteration=14500, the loss=27944.357233245813\n",
      "Current iteration=14600, the loss=27922.18252317421\n",
      "Current iteration=14700, the loss=27921.61230074917\n",
      "Current iteration=14800, the loss=27930.38664352193\n",
      "Current iteration=14900, the loss=27908.6486713666\n",
      "The loss=27911.596893648297\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import *\n",
    "\n",
    "gamma_0 = 1e-6\n",
    "max_iters_0 = 15000\n",
    "degree_0 = 2\n",
    "\n",
    "max_iters_1 = 25000\n",
    "gamma_1 = 0.9e-7\n",
    "degree_1 = 3\n",
    "\n",
    "max_iters_2 = 15000\n",
    "gamma_2 = 1e-7\n",
    "degree_2 = 3\n",
    "\n",
    "max_iters_3 = 15000\n",
    "gamma_3 = 1e-6\n",
    "degree_3 = 3\n",
    "\n",
    "X0, X1, X2, X3, indices0, indices1, indices2, indices3 = separate_data(X)\n",
    "y0 = y[indices0]\n",
    "y1 = y[indices1]\n",
    "y2 = y[indices2]\n",
    "#y3 = y[indices3]\n",
    "\n",
    "\n",
    "phi_X0 = build_poly_cos_sin_log_poly(X0, degree_0, np.array(range(1,19)))\n",
    "phi_X1 = build_poly_cos_sin_log_poly(X1, degree_1, np.array(range(1,23)))\n",
    "phi_X2 = build_poly_cos_sin_log_poly(X2, degree_2, np.array(range(1,30)))\n",
    "#phi_X3 = build_poly(X3, degree_3)\n",
    "\n",
    "w0 = reg_logistic_regression(y0, phi_X0, 0, gamma_0, max_iters_0)\n",
    "w1 = reg_logistic_regression(y1, phi_X1, 0, gamma_1, max_iters_1)\n",
    "w2 = reg_logistic_regression(y2, phi_X2, 0, gamma_2, max_iters_2)\n",
    "#w3 = reg_logistic_regression(y3, phi_X3, 0, gamma_3, max_iters_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "X0_te, X1_te, X2_te, X3_te, indices0_te, indices1_te, indices2_te, indices3_te = separate_data(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../submission6.csv' # TODO: fill in desired name of output file for submission\n",
    "ids_test0 = ids_test[indices0_te]\n",
    "ids_test1 = ids_test[indices1_te]\n",
    "ids_test2 = ids_test[indices2_te]\n",
    "#ids_test3 = ids_test[indices3_te]\n",
    "\n",
    "phi_X0_te = build_poly_cos_sin_log_poly(X0_te, degree_0, np.array(range(1,19)))\n",
    "phi_X1_te = build_poly_cos_sin_log_poly(X1_te, degree_1, np.array(range(1,23)))\n",
    "phi_X2_te = build_poly_cos_sin_log_poly(X2_te, degree_2, np.array(range(1,30)))\n",
    "#phi_X3_te = build_poly(X3_te, degree_3)\n",
    "\n",
    "pred0 = predict_labels_log_regression(w0, phi_X0_te)\n",
    "pred1 = predict_labels_log_regression(w1, phi_X1_te)\n",
    "pred2 = predict_labels_log_regression(w2, phi_X2_te)\n",
    "#pred3 = predict_labels_log_regression(w3, phi_X3_te)\n",
    "\n",
    "predictions = np.append(pred0, np.append(pred1, pred2))\n",
    "ids = np.append(ids_test0, np.append(ids_test1, ids_test2))\n",
    "\n",
    "create_csv_submission(ids, predictions, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
