{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.seterr(over='raise', invalid='raise', divide='raise');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from helpers import *\n",
    "DATA_TRAIN_PATH = '../train.csv' # TODO: download train data and supply path here \n",
    "y, X, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "X0, X1, X2, X3, indices0, indices1, indices2, indices3 = separate_data(X, y)\n",
    "y0 = y[indices0]\n",
    "y1 = y[indices1]\n",
    "y2 = y[indices2]\n",
    "y3 = y[indices3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=33037.02215576352\n",
      "Current iteration=1000, the loss=19566.28799291828\n",
      "Current iteration=2000, the loss=19407.178228340526\n",
      "Current iteration=3000, the loss=19383.457179752724\n",
      "Current iteration=4000, the loss=19379.149998930752\n",
      "The loss=19378.30739084321\n",
      "Current iteration=0, the loss=33065.48966822366\n",
      "Current iteration=1000, the loss=19903.427110723158\n",
      "Current iteration=2000, the loss=19740.027664307323\n",
      "Current iteration=3000, the loss=19714.065611711492\n",
      "Current iteration=4000, the loss=19709.157408727595\n",
      "The loss=19708.169444106512\n",
      "Training error: 19543.23841747486\n",
      "Test error: 19560.06296606074\n",
      "Classification accuracy: 0.5141074145247818\n"
     ]
    }
   ],
   "source": [
    "from toolbox import *\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 5000\n",
    "gamma = 0.000004\n",
    "lambda_ = 0\n",
    "batch_size = 100\n",
    "k = 2\n",
    "k_indices = build_k_indices(y0, k, 1)\n",
    "\n",
    "loss_tr = []\n",
    "loss_te = []\n",
    "classification_acc = []\n",
    "for k_ in range(k):\n",
    "\n",
    "    test_indices = k_indices[k_]\n",
    "    test_y, test_x = (y0[test_indices], X0[test_indices])\n",
    "\n",
    "    training_indices = np.ravel(np.delete(k_indices, k_, axis=0))\n",
    "    training_y, training_x = (y0[training_indices], X0[training_indices])\n",
    "\n",
    "    w_star = reg_logistic_regression(training_y, training_x, 0, gamma, max_iters)\n",
    "    \n",
    "    # compute classification accuracy\n",
    "    y_pred = predict_labels_log_regression(w_star, test_x)\n",
    "    \n",
    "    classification_acc.append(np.mean(np.abs(test_y + y_pred) / 2))\n",
    "\n",
    "    loss_tr.append(calculate_loss_log_likelihood(training_y, training_x, w_star))\n",
    "    loss_te.append(calculate_loss_log_likelihood(test_y, test_x, w_star))\n",
    "        \n",
    "print(\"Training error: {tr}\\nTest error: {te}\\nClassification accuracy: {cl}\".format(tr=np.mean(loss_tr), te=np.mean(loss_te), cl=np.mean(classification_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment 1\n",
    "\n",
    "- max_iters = 10000\n",
    "- gamma = 0.000004\n",
    "- lambda_ = 0\n",
    "\n",
    "Unless noted otherwise the parameters will not be changed between experiments\n",
    "\n",
    "Training error: 96889.89798817722\n",
    "\n",
    "Test error: 32303.189721963776\n",
    "\n",
    "Classification accuracy: 0.5023500000000001\n",
    "\n",
    "### experiment 2\n",
    "Binarize -999 with 0 instead of ones.\n",
    "\n",
    "Training error: 96889.89798817722\n",
    "\n",
    "Test error: 32303.189721963776\n",
    "\n",
    "Classification accuracy: 0.5023500000000001\n",
    "\n",
    "No percetible change\n",
    "\n",
    "### experiment 3\n",
    "Remove feature 29 and 9 since they are sums of the other features\n",
    "\n",
    "Training error: 96919.40308932838\n",
    "\n",
    "Test error: 32313.056842448892\n",
    "\n",
    "Classification accuracy: 0.502146\n",
    "\n",
    "No percetible change meaning that these features are not useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=63292.66128433964\n",
      "Current iteration=1000, the loss=39155.84902849459\n",
      "Current iteration=2000, the loss=39097.34272966646\n",
      "Current iteration=3000, the loss=39095.32788779734\n",
      "Current iteration=4000, the loss=39095.247837776806\n",
      "The loss=39095.244554673875\n",
      "Current iteration=0, the loss=51613.252235173306\n",
      "Current iteration=1000, the loss=41873.20322736096\n",
      "Current iteration=2000, the loss=41871.1044812114\n",
      "Current iteration=3000, the loss=41871.09276892613\n",
      "Current iteration=4000, the loss=41871.09268833713\n",
      "The loss=41871.09268775702\n",
      "Current iteration=0, the loss=33099.01208931432\n",
      "Current iteration=1000, the loss=25956.02678413775\n",
      "Current iteration=2000, the loss=25931.95216460686\n",
      "Current iteration=3000, the loss=25930.47256487579\n",
      "Current iteration=4000, the loss=25930.36354819616\n",
      "The loss=25930.35514608754\n",
      "Current iteration=0, the loss=15186.765059592964\n",
      "Current iteration=1000, the loss=11732.808674762226\n",
      "Current iteration=2000, the loss=11700.696083988594\n",
      "Current iteration=3000, the loss=11695.125874952671\n",
      "Current iteration=4000, the loss=11693.72448326399\n",
      "The loss=11693.288757770788\n"
     ]
    }
   ],
   "source": [
    "w0 = reg_logistic_regression(y0, X0, 0, gamma, max_iters)\n",
    "w1 = reg_logistic_regression(y1, X1, 0, gamma, max_iters)\n",
    "w2 = reg_logistic_regression(y2, X2, 0, gamma, max_iters)\n",
    "w3 = reg_logistic_regression(y3, X3, 0, gamma, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)\n",
    "tX_te = preprocess_inputs(tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../submission.csv' # TODO: fill in desired name of output file for submission\n",
    "y_pred = sigmoid(tX_te @ w)\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "y_pred[y_pred < 0.5] = -1\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
